{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIA 6303 - Homework 2\n",
    "\n",
    "# Eric Long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Selection:\n",
    "\n",
    "The dataset that I am using for this assignment is the popular \"Adult\" dataset. This data is used to predict whether an individual's income is above or below $50k based on the attributes provided in the dataset. The data files and column descriptions are available at http://archive.ics.uci.edu/ml/datasets/Adult.\n",
    "\n",
    "The dataset contains 48842 rows total, 45222 with complete data. This information is provided at the above website, specifically in the adult.names file. This file also serves as a good \"double-check\" for row counts after preprocessing the data.\n",
    "\n",
    "Something important to note: this dataset has already been broken up into training and testing data, using a 67/33 split. I will leave the data split this way.\n",
    "\n",
    "Another important note: the 'fnlwgt' variable is described in the adult.names file mentioned above (and is rather complex). For the purpose of this assignment this variable will be taken 'as-is'. The variable takes demographics into consideration (by residence state) so poeple with similar characteristics will have similar weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environmental Setup and Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#environmental setup\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#increase lines of output (for troubleshooting)\n",
    "pd.set_option('display.max_colwidth', 15000)\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in adult.csv\n",
    "#note - this data set is already broken into test and training sets, I will import them as such\n",
    "#also, both the train and test sets have multiple NA values coded as question marks - I will recode as 'NaN'\n",
    "#finally, the test set has a garbage first line, this will be omitted\n",
    "path = 'C:/Users/el033195/OneDrive - Cerner Corporation/PERSONAL/Data Science/BIA6303 - Predictive Models/'\n",
    "adult_train = pd.read_csv(path + 'adult.csv', header=None, na_values=\" ?\")\n",
    "adult_test = pd.read_csv(path + 'adult_test.csv', header=None, skiprows=1, na_values=\" ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#both dataframes are missing column headers\n",
    "#add column names to both dataframes\n",
    "column_list = ['age','workclass','fnlwgt','education','education-num', 'marital-status','occupation','relationship',\n",
    "               'race','sex','capital-gain','capital-loss','hours-per-week','native-country', 'income-group']\n",
    "adult_train.columns = column_list\n",
    "adult_test.columns = column_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target variable is 'income-group'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                int64\n",
       "workclass         object\n",
       "fnlwgt             int64\n",
       "education         object\n",
       "education-num      int64\n",
       "marital-status    object\n",
       "occupation        object\n",
       "relationship      object\n",
       "race              object\n",
       "sex               object\n",
       "capital-gain       int64\n",
       "capital-loss       int64\n",
       "hours-per-week     int64\n",
       "native-country    object\n",
       "income-group      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains a mix of numeric and text (categorical) data. We will handle the format of the attributes later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was also discovered that many of the values have leading spaces. Below is the code for trimming these spaces to make the data more consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I also noticed several values have leading spaces\n",
    "#will strip out spaces from both entire dataframes\n",
    "#code taken from https://stackoverflow.com/questions/40950310/strip-trim-all-strings-of-a-dataframe\n",
    "train_columns = adult_train.select_dtypes(include=['object'])\n",
    "adult_train[train_columns.columns] = train_columns.apply(lambda x: x.str.strip())\n",
    "test_columns = adult_test.select_dtypes(include=['object'])\n",
    "adult_test[test_columns.columns] = test_columns.apply(lambda x: x.str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income-group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country income-group  \n",
       "0          2174             0              40  United-States        <=50K  \n",
       "1             0             0              13  United-States        <=50K  \n",
       "2             0             0              40  United-States        <=50K  \n",
       "3             0             0              40  United-States        <=50K  \n",
       "4             0             0              40           Cuba        <=50K  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output the top 5 rows of the training dataframe\n",
    "adult_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income-group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt     education  education-num      marital-status  \\\n",
       "0   25    Private  226802          11th              7       Never-married   \n",
       "1   38    Private   89814       HS-grad              9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm             12  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college             10  Married-civ-spouse   \n",
       "4   18        NaN  103497  Some-college             10       Never-married   \n",
       "\n",
       "          occupation relationship   race     sex  capital-gain  capital-loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                NaN    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours-per-week native-country income-group  \n",
       "0              40  United-States       <=50K.  \n",
       "1              50  United-States       <=50K.  \n",
       "2              40  United-States        >50K.  \n",
       "3              40  United-States        >50K.  \n",
       "4              30  United-States       <=50K.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output the top 5 rows of the test dataframe\n",
    "adult_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  (32561, 15)\n",
      "Test set:  (16281, 15)\n"
     ]
    }
   ],
   "source": [
    "#get size of each dataset (with NaN values)\n",
    "print('Training set: ', adult_train.shape)\n",
    "print('Test set: ', adult_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset has 32561 observations and the test dataset has 16281 observations (for a total of 48842). There are 14 predictor variables and 1 outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "workclass         1836\n",
      "occupation        1843\n",
      "native-country     583\n",
      "dtype: int64\n",
      "\n",
      "Test set:\n",
      "workclass         963\n",
      "occupation        966\n",
      "native-country    274\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#get NaN counts\n",
    "nan_values = adult_train.isna().sum()\n",
    "print('Training set:')\n",
    "print(nan_values[nan_values != 0])\n",
    "nan_values = adult_test.isna().sum()\n",
    "print('\\nTest set:')\n",
    "print(nan_values[nan_values != 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are several NaN values, but we do not know how many of them occupy the same observations. However, from the information provided about the data (in adult.names) we know that there are 48842 - 45222 = 3620 observations with missing data so some of the above values must overlap. Since our data contains several thousand rows we can remove the bad rows without much impact. This would be better than assuming a value for the NaN attribute, which could skew our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are several rows that contain one or more NaN values\n",
    "#since there are about ~48k rows total, removing a few thousand will not cause issues\n",
    "#remove rows that contain missing value(s)\n",
    "adult_train.dropna(inplace=True)\n",
    "adult_test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  (30162, 15)\n",
      "Test set:  (15060, 15)\n"
     ]
    }
   ],
   "source": [
    "#get size of each dataset (without NaN values)\n",
    "print('Training set: ', adult_train.shape)\n",
    "print('Test set: ', adult_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train dataset now has 30162 observations and the test dataset now has 15060 observations (for a total of 45222). There are still 14 predictor variables and 1 outcome variable. These values are expected when compared to the adult.names file (included with the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "age                int64\n",
      "workclass         object\n",
      "fnlwgt             int64\n",
      "education         object\n",
      "education-num      int64\n",
      "marital-status    object\n",
      "occupation        object\n",
      "relationship      object\n",
      "race              object\n",
      "sex               object\n",
      "capital-gain       int64\n",
      "capital-loss       int64\n",
      "hours-per-week     int64\n",
      "native-country    object\n",
      "income-group      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#grab data types of attributes\n",
    "print('Training set:')\n",
    "print(adult_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "age                int64\n",
      "workclass         object\n",
      "fnlwgt             int64\n",
      "education         object\n",
      "education-num      int64\n",
      "marital-status    object\n",
      "occupation        object\n",
      "relationship      object\n",
      "race              object\n",
      "sex               object\n",
      "capital-gain       int64\n",
      "capital-loss       int64\n",
      "hours-per-week     int64\n",
      "native-country    object\n",
      "income-group      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#grab data types of attributes\n",
    "print('Test set:')\n",
    "print(adult_test.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are several categorical variables (data type = object) that will need to be converted into dummy variables. But first, we will perform a correlation analysis to see if any of the predictors are closely related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss',\n",
      "       'hours-per-week'],\n",
      "      dtype='object')\n",
      "(30162, 6)\n"
     ]
    }
   ],
   "source": [
    "#grab all the numeric features\n",
    "numeric_cols = adult_train.select_dtypes(include=['int64'])\n",
    "print(numeric_cols.columns)\n",
    "print(numeric_cols.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create and plot correlation matrix\n",
    "#code taken from module 2 \"Regularization_Demo\"\n",
    "corr_matrix=numeric_cols.corr()\n",
    "plt.figure(figsize=(10,10))\n",
    "mask = np.zeros_like(corr_matrix, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "sns.set_theme(font_scale=2)\n",
    "\n",
    "sns.heatmap(corr_matrix,\n",
    "           vmin=-1,\n",
    "           vmax=1,\n",
    "           cmap='coolwarm',\n",
    "           annot=True,\n",
    "           linewidths=2,\n",
    "           mask=mask)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the numeric variables are strongly (or even moderately) positively or negatively correlated. No variables will need to be dropped to handle multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the categorical variables will be converted into dummies so everything has a numeric representation. I will also do some additional cleaning - there are some inconsistencies in the outcome variables between the train and test sets. The outcome variable will also be moved to the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert non-numeric (categorical) variables into dummies\n",
    "#we will also remove the first column of the dummies (drop_first=True)\n",
    "#code taken from module 1 \"scikit_learn_intro\"\n",
    "for col in adult_train.columns[:-1]:\n",
    "\tattName = col\n",
    "\tdType = adult_train[col].dtype\n",
    "\tmissing = pd.isnull(adult_train[col]).any()\n",
    "\tuniqueCount = len(adult_train[attName].value_counts(normalize=False))\n",
    "\t# discretize (create dummies)\n",
    "\tif dType == object:\n",
    "\t\tadult_train = pd.concat([adult_train, pd.get_dummies(adult_train[col], prefix=col, drop_first=True)], axis=1)\n",
    "\t\tdel adult_train[attName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert non-numeric (categorical) variables into dummies\n",
    "#we will also remove the first column of the dummies (drop_first=True)\n",
    "#code taken from module 1 \"scikit_learn_intro\"\n",
    "for col in adult_test.columns[:-1]:\n",
    "\tattName = col\n",
    "\tdType = adult_test[col].dtype\n",
    "\tmissing = pd.isnull(adult_test[col]).any()\n",
    "\tuniqueCount = len(adult_test[attName].value_counts(normalize=False))\n",
    "\t# discretize (create dummies)\n",
    "\tif dType == object:\n",
    "\t\tadult_test = pd.concat([adult_test, pd.get_dummies(adult_test[col], prefix=col, drop_first=True)], axis=1)\n",
    "\t\tdel adult_test[attName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noticed the income-group values are different between the train and test dataset\n",
    "#will strip the period out of the test dataframe\n",
    "adult_test['income-group'] = adult_test['income-group'].str.rstrip('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next, the target variable (income-group) will be moved to the first position of the dataframe\n",
    "col_to_move='income-group'\n",
    "first_col = adult_train.pop(col_to_move)\n",
    "adult_train.insert(0, col_to_move, first_col)\n",
    "first_col = adult_test.pop(col_to_move)\n",
    "adult_test.insert(0, col_to_move, first_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income-group</th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>workclass_Self-emp-inc</th>\n",
       "      <th>workclass_Self-emp-not-inc</th>\n",
       "      <th>workclass_State-gov</th>\n",
       "      <th>workclass_Without-pay</th>\n",
       "      <th>education_11th</th>\n",
       "      <th>education_12th</th>\n",
       "      <th>education_1st-4th</th>\n",
       "      <th>education_5th-6th</th>\n",
       "      <th>education_7th-8th</th>\n",
       "      <th>education_9th</th>\n",
       "      <th>education_Assoc-acdm</th>\n",
       "      <th>education_Assoc-voc</th>\n",
       "      <th>education_Bachelors</th>\n",
       "      <th>education_Doctorate</th>\n",
       "      <th>education_HS-grad</th>\n",
       "      <th>education_Masters</th>\n",
       "      <th>education_Preschool</th>\n",
       "      <th>education_Prof-school</th>\n",
       "      <th>education_Some-college</th>\n",
       "      <th>marital-status_Married-AF-spouse</th>\n",
       "      <th>marital-status_Married-civ-spouse</th>\n",
       "      <th>marital-status_Married-spouse-absent</th>\n",
       "      <th>marital-status_Never-married</th>\n",
       "      <th>marital-status_Separated</th>\n",
       "      <th>marital-status_Widowed</th>\n",
       "      <th>occupation_Armed-Forces</th>\n",
       "      <th>occupation_Craft-repair</th>\n",
       "      <th>occupation_Exec-managerial</th>\n",
       "      <th>occupation_Farming-fishing</th>\n",
       "      <th>occupation_Handlers-cleaners</th>\n",
       "      <th>occupation_Machine-op-inspct</th>\n",
       "      <th>occupation_Other-service</th>\n",
       "      <th>occupation_Priv-house-serv</th>\n",
       "      <th>occupation_Prof-specialty</th>\n",
       "      <th>occupation_Protective-serv</th>\n",
       "      <th>occupation_Sales</th>\n",
       "      <th>occupation_Tech-support</th>\n",
       "      <th>occupation_Transport-moving</th>\n",
       "      <th>relationship_Not-in-family</th>\n",
       "      <th>relationship_Other-relative</th>\n",
       "      <th>relationship_Own-child</th>\n",
       "      <th>relationship_Unmarried</th>\n",
       "      <th>relationship_Wife</th>\n",
       "      <th>race_Asian-Pac-Islander</th>\n",
       "      <th>race_Black</th>\n",
       "      <th>race_Other</th>\n",
       "      <th>race_White</th>\n",
       "      <th>sex_Male</th>\n",
       "      <th>native-country_Canada</th>\n",
       "      <th>native-country_China</th>\n",
       "      <th>native-country_Columbia</th>\n",
       "      <th>native-country_Cuba</th>\n",
       "      <th>native-country_Dominican-Republic</th>\n",
       "      <th>native-country_Ecuador</th>\n",
       "      <th>native-country_El-Salvador</th>\n",
       "      <th>native-country_England</th>\n",
       "      <th>native-country_France</th>\n",
       "      <th>native-country_Germany</th>\n",
       "      <th>native-country_Greece</th>\n",
       "      <th>native-country_Guatemala</th>\n",
       "      <th>native-country_Haiti</th>\n",
       "      <th>native-country_Holand-Netherlands</th>\n",
       "      <th>native-country_Honduras</th>\n",
       "      <th>native-country_Hong</th>\n",
       "      <th>native-country_Hungary</th>\n",
       "      <th>native-country_India</th>\n",
       "      <th>native-country_Iran</th>\n",
       "      <th>native-country_Ireland</th>\n",
       "      <th>native-country_Italy</th>\n",
       "      <th>native-country_Jamaica</th>\n",
       "      <th>native-country_Japan</th>\n",
       "      <th>native-country_Laos</th>\n",
       "      <th>native-country_Mexico</th>\n",
       "      <th>native-country_Nicaragua</th>\n",
       "      <th>native-country_Outlying-US(Guam-USVI-etc)</th>\n",
       "      <th>native-country_Peru</th>\n",
       "      <th>native-country_Philippines</th>\n",
       "      <th>native-country_Poland</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>39</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>50</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>38</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>53</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>28</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  income-group  age  fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "0        <=50K   39   77516             13          2174             0   \n",
       "1        <=50K   50   83311             13             0             0   \n",
       "2        <=50K   38  215646              9             0             0   \n",
       "3        <=50K   53  234721              7             0             0   \n",
       "4        <=50K   28  338409             13             0             0   \n",
       "\n",
       "   hours-per-week  workclass_Local-gov  workclass_Private  \\\n",
       "0              40                    0                  0   \n",
       "1              13                    0                  0   \n",
       "2              40                    0                  1   \n",
       "3              40                    0                  1   \n",
       "4              40                    0                  1   \n",
       "\n",
       "   workclass_Self-emp-inc  workclass_Self-emp-not-inc  workclass_State-gov  \\\n",
       "0                       0                           0                    1   \n",
       "1                       0                           1                    0   \n",
       "2                       0                           0                    0   \n",
       "3                       0                           0                    0   \n",
       "4                       0                           0                    0   \n",
       "\n",
       "   workclass_Without-pay  education_11th  education_12th  education_1st-4th  \\\n",
       "0                      0               0               0                  0   \n",
       "1                      0               0               0                  0   \n",
       "2                      0               0               0                  0   \n",
       "3                      0               1               0                  0   \n",
       "4                      0               0               0                  0   \n",
       "\n",
       "   education_5th-6th  education_7th-8th  education_9th  education_Assoc-acdm  \\\n",
       "0                  0                  0              0                     0   \n",
       "1                  0                  0              0                     0   \n",
       "2                  0                  0              0                     0   \n",
       "3                  0                  0              0                     0   \n",
       "4                  0                  0              0                     0   \n",
       "\n",
       "   education_Assoc-voc  education_Bachelors  education_Doctorate  \\\n",
       "0                    0                    1                    0   \n",
       "1                    0                    1                    0   \n",
       "2                    0                    0                    0   \n",
       "3                    0                    0                    0   \n",
       "4                    0                    1                    0   \n",
       "\n",
       "   education_HS-grad  education_Masters  education_Preschool  \\\n",
       "0                  0                  0                    0   \n",
       "1                  0                  0                    0   \n",
       "2                  1                  0                    0   \n",
       "3                  0                  0                    0   \n",
       "4                  0                  0                    0   \n",
       "\n",
       "   education_Prof-school  education_Some-college  \\\n",
       "0                      0                       0   \n",
       "1                      0                       0   \n",
       "2                      0                       0   \n",
       "3                      0                       0   \n",
       "4                      0                       0   \n",
       "\n",
       "   marital-status_Married-AF-spouse  marital-status_Married-civ-spouse  \\\n",
       "0                                 0                                  0   \n",
       "1                                 0                                  1   \n",
       "2                                 0                                  0   \n",
       "3                                 0                                  1   \n",
       "4                                 0                                  1   \n",
       "\n",
       "   marital-status_Married-spouse-absent  marital-status_Never-married  \\\n",
       "0                                     0                             1   \n",
       "1                                     0                             0   \n",
       "2                                     0                             0   \n",
       "3                                     0                             0   \n",
       "4                                     0                             0   \n",
       "\n",
       "   marital-status_Separated  marital-status_Widowed  occupation_Armed-Forces  \\\n",
       "0                         0                       0                        0   \n",
       "1                         0                       0                        0   \n",
       "2                         0                       0                        0   \n",
       "3                         0                       0                        0   \n",
       "4                         0                       0                        0   \n",
       "\n",
       "   occupation_Craft-repair  occupation_Exec-managerial  \\\n",
       "0                        0                           0   \n",
       "1                        0                           1   \n",
       "2                        0                           0   \n",
       "3                        0                           0   \n",
       "4                        0                           0   \n",
       "\n",
       "   occupation_Farming-fishing  occupation_Handlers-cleaners  \\\n",
       "0                           0                             0   \n",
       "1                           0                             0   \n",
       "2                           0                             1   \n",
       "3                           0                             1   \n",
       "4                           0                             0   \n",
       "\n",
       "   occupation_Machine-op-inspct  occupation_Other-service  \\\n",
       "0                             0                         0   \n",
       "1                             0                         0   \n",
       "2                             0                         0   \n",
       "3                             0                         0   \n",
       "4                             0                         0   \n",
       "\n",
       "   occupation_Priv-house-serv  occupation_Prof-specialty  \\\n",
       "0                           0                          0   \n",
       "1                           0                          0   \n",
       "2                           0                          0   \n",
       "3                           0                          0   \n",
       "4                           0                          1   \n",
       "\n",
       "   occupation_Protective-serv  occupation_Sales  occupation_Tech-support  \\\n",
       "0                           0                 0                        0   \n",
       "1                           0                 0                        0   \n",
       "2                           0                 0                        0   \n",
       "3                           0                 0                        0   \n",
       "4                           0                 0                        0   \n",
       "\n",
       "   occupation_Transport-moving  relationship_Not-in-family  \\\n",
       "0                            0                           1   \n",
       "1                            0                           0   \n",
       "2                            0                           1   \n",
       "3                            0                           0   \n",
       "4                            0                           0   \n",
       "\n",
       "   relationship_Other-relative  relationship_Own-child  \\\n",
       "0                            0                       0   \n",
       "1                            0                       0   \n",
       "2                            0                       0   \n",
       "3                            0                       0   \n",
       "4                            0                       0   \n",
       "\n",
       "   relationship_Unmarried  relationship_Wife  race_Asian-Pac-Islander  \\\n",
       "0                       0                  0                        0   \n",
       "1                       0                  0                        0   \n",
       "2                       0                  0                        0   \n",
       "3                       0                  0                        0   \n",
       "4                       0                  1                        0   \n",
       "\n",
       "   race_Black  race_Other  race_White  sex_Male  native-country_Canada  \\\n",
       "0           0           0           1         1                      0   \n",
       "1           0           0           1         1                      0   \n",
       "2           0           0           1         1                      0   \n",
       "3           1           0           0         1                      0   \n",
       "4           1           0           0         0                      0   \n",
       "\n",
       "   native-country_China  native-country_Columbia  native-country_Cuba  \\\n",
       "0                     0                        0                    0   \n",
       "1                     0                        0                    0   \n",
       "2                     0                        0                    0   \n",
       "3                     0                        0                    0   \n",
       "4                     0                        0                    1   \n",
       "\n",
       "   native-country_Dominican-Republic  native-country_Ecuador  \\\n",
       "0                                  0                       0   \n",
       "1                                  0                       0   \n",
       "2                                  0                       0   \n",
       "3                                  0                       0   \n",
       "4                                  0                       0   \n",
       "\n",
       "   native-country_El-Salvador  native-country_England  native-country_France  \\\n",
       "0                           0                       0                      0   \n",
       "1                           0                       0                      0   \n",
       "2                           0                       0                      0   \n",
       "3                           0                       0                      0   \n",
       "4                           0                       0                      0   \n",
       "\n",
       "   native-country_Germany  native-country_Greece  native-country_Guatemala  \\\n",
       "0                       0                      0                         0   \n",
       "1                       0                      0                         0   \n",
       "2                       0                      0                         0   \n",
       "3                       0                      0                         0   \n",
       "4                       0                      0                         0   \n",
       "\n",
       "   native-country_Haiti  native-country_Holand-Netherlands  \\\n",
       "0                     0                                  0   \n",
       "1                     0                                  0   \n",
       "2                     0                                  0   \n",
       "3                     0                                  0   \n",
       "4                     0                                  0   \n",
       "\n",
       "   native-country_Honduras  native-country_Hong  native-country_Hungary  \\\n",
       "0                        0                    0                       0   \n",
       "1                        0                    0                       0   \n",
       "2                        0                    0                       0   \n",
       "3                        0                    0                       0   \n",
       "4                        0                    0                       0   \n",
       "\n",
       "   native-country_India  native-country_Iran  native-country_Ireland  \\\n",
       "0                     0                    0                       0   \n",
       "1                     0                    0                       0   \n",
       "2                     0                    0                       0   \n",
       "3                     0                    0                       0   \n",
       "4                     0                    0                       0   \n",
       "\n",
       "   native-country_Italy  native-country_Jamaica  native-country_Japan  \\\n",
       "0                     0                       0                     0   \n",
       "1                     0                       0                     0   \n",
       "2                     0                       0                     0   \n",
       "3                     0                       0                     0   \n",
       "4                     0                       0                     0   \n",
       "\n",
       "   native-country_Laos  native-country_Mexico  native-country_Nicaragua  \\\n",
       "0                    0                      0                         0   \n",
       "1                    0                      0                         0   \n",
       "2                    0                      0                         0   \n",
       "3                    0                      0                         0   \n",
       "4                    0                      0                         0   \n",
       "\n",
       "   native-country_Outlying-US(Guam-USVI-etc)  native-country_Peru  \\\n",
       "0                                          0                    0   \n",
       "1                                          0                    0   \n",
       "2                                          0                    0   \n",
       "3                                          0                    0   \n",
       "4                                          0                    0   \n",
       "\n",
       "   native-country_Philippines  native-country_Poland  native-country_Portugal  \\\n",
       "0                           0                      0                        0   \n",
       "1                           0                      0                        0   \n",
       "2                           0                      0                        0   \n",
       "3                           0                      0                        0   \n",
       "4                           0                      0                        0   \n",
       "\n",
       "   native-country_Puerto-Rico  native-country_Scotland  native-country_South  \\\n",
       "0                           0                        0                     0   \n",
       "1                           0                        0                     0   \n",
       "2                           0                        0                     0   \n",
       "3                           0                        0                     0   \n",
       "4                           0                        0                     0   \n",
       "\n",
       "   native-country_Taiwan  native-country_Thailand  \\\n",
       "0                      0                        0   \n",
       "1                      0                        0   \n",
       "2                      0                        0   \n",
       "3                      0                        0   \n",
       "4                      0                        0   \n",
       "\n",
       "   native-country_Trinadad&Tobago  native-country_United-States  \\\n",
       "0                               0                             1   \n",
       "1                               0                             1   \n",
       "2                               0                             1   \n",
       "3                               0                             1   \n",
       "4                               0                             0   \n",
       "\n",
       "   native-country_Vietnam  native-country_Yugoslavia  \n",
       "0                       0                          0  \n",
       "1                       0                          0  \n",
       "2                       0                          0  \n",
       "3                       0                          0  \n",
       "4                       0                          0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confirm dummy variables created\n",
    "adult_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income-group</th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>workclass_Self-emp-inc</th>\n",
       "      <th>workclass_Self-emp-not-inc</th>\n",
       "      <th>workclass_State-gov</th>\n",
       "      <th>workclass_Without-pay</th>\n",
       "      <th>education_11th</th>\n",
       "      <th>education_12th</th>\n",
       "      <th>education_1st-4th</th>\n",
       "      <th>education_5th-6th</th>\n",
       "      <th>education_7th-8th</th>\n",
       "      <th>education_9th</th>\n",
       "      <th>education_Assoc-acdm</th>\n",
       "      <th>education_Assoc-voc</th>\n",
       "      <th>education_Bachelors</th>\n",
       "      <th>education_Doctorate</th>\n",
       "      <th>education_HS-grad</th>\n",
       "      <th>education_Masters</th>\n",
       "      <th>education_Preschool</th>\n",
       "      <th>education_Prof-school</th>\n",
       "      <th>education_Some-college</th>\n",
       "      <th>marital-status_Married-AF-spouse</th>\n",
       "      <th>marital-status_Married-civ-spouse</th>\n",
       "      <th>marital-status_Married-spouse-absent</th>\n",
       "      <th>marital-status_Never-married</th>\n",
       "      <th>marital-status_Separated</th>\n",
       "      <th>marital-status_Widowed</th>\n",
       "      <th>occupation_Armed-Forces</th>\n",
       "      <th>occupation_Craft-repair</th>\n",
       "      <th>occupation_Exec-managerial</th>\n",
       "      <th>occupation_Farming-fishing</th>\n",
       "      <th>occupation_Handlers-cleaners</th>\n",
       "      <th>occupation_Machine-op-inspct</th>\n",
       "      <th>occupation_Other-service</th>\n",
       "      <th>occupation_Priv-house-serv</th>\n",
       "      <th>occupation_Prof-specialty</th>\n",
       "      <th>occupation_Protective-serv</th>\n",
       "      <th>occupation_Sales</th>\n",
       "      <th>occupation_Tech-support</th>\n",
       "      <th>occupation_Transport-moving</th>\n",
       "      <th>relationship_Not-in-family</th>\n",
       "      <th>relationship_Other-relative</th>\n",
       "      <th>relationship_Own-child</th>\n",
       "      <th>relationship_Unmarried</th>\n",
       "      <th>relationship_Wife</th>\n",
       "      <th>race_Asian-Pac-Islander</th>\n",
       "      <th>race_Black</th>\n",
       "      <th>race_Other</th>\n",
       "      <th>race_White</th>\n",
       "      <th>sex_Male</th>\n",
       "      <th>native-country_Canada</th>\n",
       "      <th>native-country_China</th>\n",
       "      <th>native-country_Columbia</th>\n",
       "      <th>native-country_Cuba</th>\n",
       "      <th>native-country_Dominican-Republic</th>\n",
       "      <th>native-country_Ecuador</th>\n",
       "      <th>native-country_El-Salvador</th>\n",
       "      <th>native-country_England</th>\n",
       "      <th>native-country_France</th>\n",
       "      <th>native-country_Germany</th>\n",
       "      <th>native-country_Greece</th>\n",
       "      <th>native-country_Guatemala</th>\n",
       "      <th>native-country_Haiti</th>\n",
       "      <th>native-country_Honduras</th>\n",
       "      <th>native-country_Hong</th>\n",
       "      <th>native-country_Hungary</th>\n",
       "      <th>native-country_India</th>\n",
       "      <th>native-country_Iran</th>\n",
       "      <th>native-country_Ireland</th>\n",
       "      <th>native-country_Italy</th>\n",
       "      <th>native-country_Jamaica</th>\n",
       "      <th>native-country_Japan</th>\n",
       "      <th>native-country_Laos</th>\n",
       "      <th>native-country_Mexico</th>\n",
       "      <th>native-country_Nicaragua</th>\n",
       "      <th>native-country_Outlying-US(Guam-USVI-etc)</th>\n",
       "      <th>native-country_Peru</th>\n",
       "      <th>native-country_Philippines</th>\n",
       "      <th>native-country_Poland</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>25</td>\n",
       "      <td>226802</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>38</td>\n",
       "      <td>89814</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>28</td>\n",
       "      <td>336951</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>44</td>\n",
       "      <td>160323</td>\n",
       "      <td>10</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>34</td>\n",
       "      <td>198693</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  income-group  age  fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "0        <=50K   25  226802              7             0             0   \n",
       "1        <=50K   38   89814              9             0             0   \n",
       "2         >50K   28  336951             12             0             0   \n",
       "3         >50K   44  160323             10          7688             0   \n",
       "5        <=50K   34  198693              6             0             0   \n",
       "\n",
       "   hours-per-week  workclass_Local-gov  workclass_Private  \\\n",
       "0              40                    0                  1   \n",
       "1              50                    0                  1   \n",
       "2              40                    1                  0   \n",
       "3              40                    0                  1   \n",
       "5              30                    0                  1   \n",
       "\n",
       "   workclass_Self-emp-inc  workclass_Self-emp-not-inc  workclass_State-gov  \\\n",
       "0                       0                           0                    0   \n",
       "1                       0                           0                    0   \n",
       "2                       0                           0                    0   \n",
       "3                       0                           0                    0   \n",
       "5                       0                           0                    0   \n",
       "\n",
       "   workclass_Without-pay  education_11th  education_12th  education_1st-4th  \\\n",
       "0                      0               1               0                  0   \n",
       "1                      0               0               0                  0   \n",
       "2                      0               0               0                  0   \n",
       "3                      0               0               0                  0   \n",
       "5                      0               0               0                  0   \n",
       "\n",
       "   education_5th-6th  education_7th-8th  education_9th  education_Assoc-acdm  \\\n",
       "0                  0                  0              0                     0   \n",
       "1                  0                  0              0                     0   \n",
       "2                  0                  0              0                     1   \n",
       "3                  0                  0              0                     0   \n",
       "5                  0                  0              0                     0   \n",
       "\n",
       "   education_Assoc-voc  education_Bachelors  education_Doctorate  \\\n",
       "0                    0                    0                    0   \n",
       "1                    0                    0                    0   \n",
       "2                    0                    0                    0   \n",
       "3                    0                    0                    0   \n",
       "5                    0                    0                    0   \n",
       "\n",
       "   education_HS-grad  education_Masters  education_Preschool  \\\n",
       "0                  0                  0                    0   \n",
       "1                  1                  0                    0   \n",
       "2                  0                  0                    0   \n",
       "3                  0                  0                    0   \n",
       "5                  0                  0                    0   \n",
       "\n",
       "   education_Prof-school  education_Some-college  \\\n",
       "0                      0                       0   \n",
       "1                      0                       0   \n",
       "2                      0                       0   \n",
       "3                      0                       1   \n",
       "5                      0                       0   \n",
       "\n",
       "   marital-status_Married-AF-spouse  marital-status_Married-civ-spouse  \\\n",
       "0                                 0                                  0   \n",
       "1                                 0                                  1   \n",
       "2                                 0                                  1   \n",
       "3                                 0                                  1   \n",
       "5                                 0                                  0   \n",
       "\n",
       "   marital-status_Married-spouse-absent  marital-status_Never-married  \\\n",
       "0                                     0                             1   \n",
       "1                                     0                             0   \n",
       "2                                     0                             0   \n",
       "3                                     0                             0   \n",
       "5                                     0                             1   \n",
       "\n",
       "   marital-status_Separated  marital-status_Widowed  occupation_Armed-Forces  \\\n",
       "0                         0                       0                        0   \n",
       "1                         0                       0                        0   \n",
       "2                         0                       0                        0   \n",
       "3                         0                       0                        0   \n",
       "5                         0                       0                        0   \n",
       "\n",
       "   occupation_Craft-repair  occupation_Exec-managerial  \\\n",
       "0                        0                           0   \n",
       "1                        0                           0   \n",
       "2                        0                           0   \n",
       "3                        0                           0   \n",
       "5                        0                           0   \n",
       "\n",
       "   occupation_Farming-fishing  occupation_Handlers-cleaners  \\\n",
       "0                           0                             0   \n",
       "1                           1                             0   \n",
       "2                           0                             0   \n",
       "3                           0                             0   \n",
       "5                           0                             0   \n",
       "\n",
       "   occupation_Machine-op-inspct  occupation_Other-service  \\\n",
       "0                             1                         0   \n",
       "1                             0                         0   \n",
       "2                             0                         0   \n",
       "3                             1                         0   \n",
       "5                             0                         1   \n",
       "\n",
       "   occupation_Priv-house-serv  occupation_Prof-specialty  \\\n",
       "0                           0                          0   \n",
       "1                           0                          0   \n",
       "2                           0                          0   \n",
       "3                           0                          0   \n",
       "5                           0                          0   \n",
       "\n",
       "   occupation_Protective-serv  occupation_Sales  occupation_Tech-support  \\\n",
       "0                           0                 0                        0   \n",
       "1                           0                 0                        0   \n",
       "2                           1                 0                        0   \n",
       "3                           0                 0                        0   \n",
       "5                           0                 0                        0   \n",
       "\n",
       "   occupation_Transport-moving  relationship_Not-in-family  \\\n",
       "0                            0                           0   \n",
       "1                            0                           0   \n",
       "2                            0                           0   \n",
       "3                            0                           0   \n",
       "5                            0                           1   \n",
       "\n",
       "   relationship_Other-relative  relationship_Own-child  \\\n",
       "0                            0                       1   \n",
       "1                            0                       0   \n",
       "2                            0                       0   \n",
       "3                            0                       0   \n",
       "5                            0                       0   \n",
       "\n",
       "   relationship_Unmarried  relationship_Wife  race_Asian-Pac-Islander  \\\n",
       "0                       0                  0                        0   \n",
       "1                       0                  0                        0   \n",
       "2                       0                  0                        0   \n",
       "3                       0                  0                        0   \n",
       "5                       0                  0                        0   \n",
       "\n",
       "   race_Black  race_Other  race_White  sex_Male  native-country_Canada  \\\n",
       "0           1           0           0         1                      0   \n",
       "1           0           0           1         1                      0   \n",
       "2           0           0           1         1                      0   \n",
       "3           1           0           0         1                      0   \n",
       "5           0           0           1         1                      0   \n",
       "\n",
       "   native-country_China  native-country_Columbia  native-country_Cuba  \\\n",
       "0                     0                        0                    0   \n",
       "1                     0                        0                    0   \n",
       "2                     0                        0                    0   \n",
       "3                     0                        0                    0   \n",
       "5                     0                        0                    0   \n",
       "\n",
       "   native-country_Dominican-Republic  native-country_Ecuador  \\\n",
       "0                                  0                       0   \n",
       "1                                  0                       0   \n",
       "2                                  0                       0   \n",
       "3                                  0                       0   \n",
       "5                                  0                       0   \n",
       "\n",
       "   native-country_El-Salvador  native-country_England  native-country_France  \\\n",
       "0                           0                       0                      0   \n",
       "1                           0                       0                      0   \n",
       "2                           0                       0                      0   \n",
       "3                           0                       0                      0   \n",
       "5                           0                       0                      0   \n",
       "\n",
       "   native-country_Germany  native-country_Greece  native-country_Guatemala  \\\n",
       "0                       0                      0                         0   \n",
       "1                       0                      0                         0   \n",
       "2                       0                      0                         0   \n",
       "3                       0                      0                         0   \n",
       "5                       0                      0                         0   \n",
       "\n",
       "   native-country_Haiti  native-country_Honduras  native-country_Hong  \\\n",
       "0                     0                        0                    0   \n",
       "1                     0                        0                    0   \n",
       "2                     0                        0                    0   \n",
       "3                     0                        0                    0   \n",
       "5                     0                        0                    0   \n",
       "\n",
       "   native-country_Hungary  native-country_India  native-country_Iran  \\\n",
       "0                       0                     0                    0   \n",
       "1                       0                     0                    0   \n",
       "2                       0                     0                    0   \n",
       "3                       0                     0                    0   \n",
       "5                       0                     0                    0   \n",
       "\n",
       "   native-country_Ireland  native-country_Italy  native-country_Jamaica  \\\n",
       "0                       0                     0                       0   \n",
       "1                       0                     0                       0   \n",
       "2                       0                     0                       0   \n",
       "3                       0                     0                       0   \n",
       "5                       0                     0                       0   \n",
       "\n",
       "   native-country_Japan  native-country_Laos  native-country_Mexico  \\\n",
       "0                     0                    0                      0   \n",
       "1                     0                    0                      0   \n",
       "2                     0                    0                      0   \n",
       "3                     0                    0                      0   \n",
       "5                     0                    0                      0   \n",
       "\n",
       "   native-country_Nicaragua  native-country_Outlying-US(Guam-USVI-etc)  \\\n",
       "0                         0                                          0   \n",
       "1                         0                                          0   \n",
       "2                         0                                          0   \n",
       "3                         0                                          0   \n",
       "5                         0                                          0   \n",
       "\n",
       "   native-country_Peru  native-country_Philippines  native-country_Poland  \\\n",
       "0                    0                           0                      0   \n",
       "1                    0                           0                      0   \n",
       "2                    0                           0                      0   \n",
       "3                    0                           0                      0   \n",
       "5                    0                           0                      0   \n",
       "\n",
       "   native-country_Portugal  native-country_Puerto-Rico  \\\n",
       "0                        0                           0   \n",
       "1                        0                           0   \n",
       "2                        0                           0   \n",
       "3                        0                           0   \n",
       "5                        0                           0   \n",
       "\n",
       "   native-country_Scotland  native-country_South  native-country_Taiwan  \\\n",
       "0                        0                     0                      0   \n",
       "1                        0                     0                      0   \n",
       "2                        0                     0                      0   \n",
       "3                        0                     0                      0   \n",
       "5                        0                     0                      0   \n",
       "\n",
       "   native-country_Thailand  native-country_Trinadad&Tobago  \\\n",
       "0                        0                               0   \n",
       "1                        0                               0   \n",
       "2                        0                               0   \n",
       "3                        0                               0   \n",
       "5                        0                               0   \n",
       "\n",
       "   native-country_United-States  native-country_Vietnam  \\\n",
       "0                             1                       0   \n",
       "1                             1                       0   \n",
       "2                             1                       0   \n",
       "3                             1                       0   \n",
       "5                             1                       0   \n",
       "\n",
       "   native-country_Yugoslavia  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "5                          0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confirm dataframe looks as expected\n",
    "adult_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  ['<=50K' '>50K']\n",
      "Test set:  ['<=50K' '>50K']\n"
     ]
    }
   ],
   "source": [
    "#confirm target variables match\n",
    "print('Training set: ', adult_train['income-group'].unique())\n",
    "print('Test set: ', adult_test['income-group'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  (30162, 97)\n",
      "Test set:  (15060, 96)\n"
     ]
    }
   ],
   "source": [
    "#get new size of each dataset (without NaN values, dummies added)\n",
    "print('Training set: ', adult_train.shape)\n",
    "print('Test set: ', adult_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training set has one additional predictor variable - this is likely due to one of the dummies not be present in the test set. After scrolling through, I can see that we're missing the 'native-country_Holand-Netherlands' variable. To make the datasets match I will re-add a column with all 0 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get index of the 'native-country_Holand-Netherlands' column from the training set\n",
    "#code taken from https://stackoverflow.com/questions/13021654/get-column-index-from-column-name-in-python-pandas\n",
    "adult_train.columns.get_loc('native-country_Holand-Netherlands')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add new column to test dataframe\n",
    "#set all the values = 0\n",
    "adult_test.insert(70, 'native-country_Holand-Netherlands', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  (30162, 97)\n",
      "Test set:  (15060, 97)\n"
     ]
    }
   ],
   "source": [
    "#confirm sizes now match\n",
    "print('Training set: ', adult_train.shape)\n",
    "print('Test set: ', adult_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been cleaned up we can do some exploratory data analysis. First I will grab basic stats about the numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30162.000000</td>\n",
       "      <td>3.016200e+04</td>\n",
       "      <td>30162.000000</td>\n",
       "      <td>30162.000000</td>\n",
       "      <td>30162.000000</td>\n",
       "      <td>30162.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.437902</td>\n",
       "      <td>1.897938e+05</td>\n",
       "      <td>10.121312</td>\n",
       "      <td>1092.007858</td>\n",
       "      <td>88.372489</td>\n",
       "      <td>40.931238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.134665</td>\n",
       "      <td>1.056530e+05</td>\n",
       "      <td>2.549995</td>\n",
       "      <td>7406.346497</td>\n",
       "      <td>404.298370</td>\n",
       "      <td>11.979984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.376900e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.176272e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.784250e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>2.376285e+05</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.484705e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "count  30162.000000  3.016200e+04   30162.000000  30162.000000  30162.000000   \n",
       "mean      38.437902  1.897938e+05      10.121312   1092.007858     88.372489   \n",
       "std       13.134665  1.056530e+05       2.549995   7406.346497    404.298370   \n",
       "min       17.000000  1.376900e+04       1.000000      0.000000      0.000000   \n",
       "25%       28.000000  1.176272e+05       9.000000      0.000000      0.000000   \n",
       "50%       37.000000  1.784250e+05      10.000000      0.000000      0.000000   \n",
       "75%       47.000000  2.376285e+05      13.000000      0.000000      0.000000   \n",
       "max       90.000000  1.484705e+06      16.000000  99999.000000   4356.000000   \n",
       "\n",
       "       hours-per-week  \n",
       "count    30162.000000  \n",
       "mean        40.931238  \n",
       "std         11.979984  \n",
       "min          1.000000  \n",
       "25%         40.000000  \n",
       "50%         40.000000  \n",
       "75%         45.000000  \n",
       "max         99.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training set stats (numeric variables)\n",
    "adult_train[numeric_cols.columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15060.000000</td>\n",
       "      <td>1.506000e+04</td>\n",
       "      <td>15060.000000</td>\n",
       "      <td>15060.000000</td>\n",
       "      <td>15060.000000</td>\n",
       "      <td>15060.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.768327</td>\n",
       "      <td>1.896164e+05</td>\n",
       "      <td>10.112749</td>\n",
       "      <td>1120.301594</td>\n",
       "      <td>89.041899</td>\n",
       "      <td>40.951594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.380676</td>\n",
       "      <td>1.056150e+05</td>\n",
       "      <td>2.558727</td>\n",
       "      <td>7703.181842</td>\n",
       "      <td>406.283245</td>\n",
       "      <td>12.062831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.349200e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.166550e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.779550e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.385888e+05</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.490400e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>3770.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "count  15060.000000  1.506000e+04   15060.000000  15060.000000  15060.000000   \n",
       "mean      38.768327  1.896164e+05      10.112749   1120.301594     89.041899   \n",
       "std       13.380676  1.056150e+05       2.558727   7703.181842    406.283245   \n",
       "min       17.000000  1.349200e+04       1.000000      0.000000      0.000000   \n",
       "25%       28.000000  1.166550e+05       9.000000      0.000000      0.000000   \n",
       "50%       37.000000  1.779550e+05      10.000000      0.000000      0.000000   \n",
       "75%       48.000000  2.385888e+05      13.000000      0.000000      0.000000   \n",
       "max       90.000000  1.490400e+06      16.000000  99999.000000   3770.000000   \n",
       "\n",
       "       hours-per-week  \n",
       "count    15060.000000  \n",
       "mean        40.951594  \n",
       "std         12.062831  \n",
       "min          1.000000  \n",
       "25%         40.000000  \n",
       "50%         40.000000  \n",
       "75%         45.000000  \n",
       "max         99.000000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test set stats (numeric variables)\n",
    "adult_test[numeric_cols.columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the mean and standard deviations of the numeric variables are pretty similar between the training and testing data sets - this is good because it indicates the sample populations are similar. The ages of the training and test sets range between 17 and 90. The hours worked per week also has the same range for both sets - 1 to 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income-group\n",
      "<=50K           34014\n",
      ">50K            11208\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot out distribution of income-group\n",
    "#first we need to combine the total income-group counts into one dataframe\n",
    "total_inc = pd.DataFrame(columns=['income-group'])\n",
    "total_inc['income-group'] = total_inc['income-group'].append([adult_train['income-group'],adult_test['income-group']],\n",
    "                                                             ignore_index=True)\n",
    "#print(total_inc.shape)\n",
    "#next, plot the income-group counts\n",
    "total_inc.value_counts().plot(kind='bar')\n",
    "print(total_inc.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is not very balanced - the greater-than-50K income group (>50K) contains 11208/45222 = 24.8% of the observations. The less-than-50K group (>=50K) contains 34014/45222 = 75.2% of the observations. This will need to be considered when evaluating the effectiveness of the classification models. Instead of looking at model accuracy it will probably be better to look at a combination of model recall and precision. However, we still need an accuracy greater than 75.2%, since this is the distribution of the current data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: Grid search was not used due to how long it was taking for models to complete - some models were taking 2+ hours. Instead different parameters were manually tested and noted in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30162, 96)\n",
      "(30162,)\n",
      "(15060, 96)\n",
      "(15060,)\n"
     ]
    }
   ],
   "source": [
    "#first, we will create the training and test sets\n",
    "#since my dataset was already divided I will not require the train_test_split package\n",
    "features_train = adult_train.iloc[:,1:].values\n",
    "features_test = adult_test.iloc[:,1:].values\n",
    "target_train = adult_train.iloc[:,0].values\n",
    "target_test = adult_test.iloc[:,0].values\n",
    "\n",
    "print(features_train.shape)\n",
    "print(target_train.shape)\n",
    "print(features_test.shape)\n",
    "print(target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the KNN algorithm is based on distance the data must first be scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy Score: 0.8187915006640106\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.87      0.90      0.88     11360\n",
      "        >50K       0.65      0.57      0.61      3700\n",
      "\n",
      "    accuracy                           0.82     15060\n",
      "   macro avg       0.76      0.74      0.75     15060\n",
      "weighted avg       0.81      0.82      0.81     15060\n",
      "\n",
      "[[10214  1146]\n",
      " [ 1583  2117]]\n",
      "True Negatives:  10214\n",
      "False Positives:  1146\n",
      "False Negatives:  1583\n",
      "True Positives:  2117\n"
     ]
    }
   ],
   "source": [
    "#KNN model 1 - default parameters\n",
    "#n_neighbors = 5\n",
    "\n",
    "#import packages\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#build\n",
    "clf_knn = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "\n",
    "#train\n",
    "clf_knn = clf_knn.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_knn = clf_knn.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"KNN Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_knn))\n",
    "print(metrics.classification_report(target_test, target_predicted_knn))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_knn))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_knn).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has a pretty good accuracy (82%) but has poor recall and precision for the positive class (>50K). Here the model only identifies 57% of >50K cases and predicts only 65% correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy Score: 0.8204515272244356\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.87      0.90      0.88     11360\n",
      "        >50K       0.65      0.57      0.61      3700\n",
      "\n",
      "    accuracy                           0.82     15060\n",
      "   macro avg       0.76      0.74      0.75     15060\n",
      "weighted avg       0.81      0.82      0.82     15060\n",
      "\n",
      "[[10237  1123]\n",
      " [ 1581  2119]]\n",
      "True Negatives:  10237\n",
      "False Positives:  1123\n",
      "False Negatives:  1581\n",
      "True Positives:  2119\n"
     ]
    }
   ],
   "source": [
    "#KNN model 2\n",
    "#n_neighbors = 7\n",
    "\n",
    "#build\n",
    "clf_knn = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=7))\n",
    "\n",
    "#train\n",
    "clf_knn = clf_knn.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_knn = clf_knn.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"KNN Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_knn))\n",
    "print(metrics.classification_report(target_test, target_predicted_knn))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_knn))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_knn).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of neighbors was increased from 5 to 7 for the second model. The overall accuracy increased but the recall and precision for the positive class remained the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy Score: 0.8085657370517928\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.86      0.89      0.87     11360\n",
      "        >50K       0.62      0.57      0.59      3700\n",
      "\n",
      "    accuracy                           0.81     15060\n",
      "   macro avg       0.74      0.73      0.73     15060\n",
      "weighted avg       0.80      0.81      0.81     15060\n",
      "\n",
      "[[10082  1278]\n",
      " [ 1605  2095]]\n",
      "True Negatives:  10082\n",
      "False Positives:  1278\n",
      "False Negatives:  1605\n",
      "True Positives:  2095\n"
     ]
    }
   ],
   "source": [
    "#KNN model 3\n",
    "#n_neighbors = 3\n",
    "\n",
    "#build\n",
    "clf_knn = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=3))\n",
    "\n",
    "#train\n",
    "clf_knn = clf_knn.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_knn = clf_knn.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"KNN Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_knn))\n",
    "print(metrics.classification_report(target_test, target_predicted_knn))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_knn))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_knn).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the number of neighbors was dropped down to 3. This resulted in a worse model than either of the previous two. Reducing the number of neighbors too low also risks overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since decision trees are not linear/distance based scaling is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT Accuracy Score: 0.8090969455511288\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.87      0.87     11360\n",
      "        >50K       0.61      0.62      0.62      3700\n",
      "\n",
      "    accuracy                           0.81     15060\n",
      "   macro avg       0.74      0.75      0.74     15060\n",
      "weighted avg       0.81      0.81      0.81     15060\n",
      "\n",
      "[[9888 1472]\n",
      " [1403 2297]]\n",
      "True Negatives:  9888\n",
      "False Positives:  1472\n",
      "False Negatives:  1403\n",
      "True Positives:  2297\n",
      "DT depth:  51\n"
     ]
    }
   ],
   "source": [
    "#DT model 1 - default parameters\n",
    "\n",
    "#import packages\n",
    "from sklearn import tree\n",
    "\n",
    "#build\n",
    "clf_dt = tree.DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "#train\n",
    "clf_dt = clf_dt.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_dt = clf_dt.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"DT Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_dt))\n",
    "print(metrics.classification_report(target_test, target_predicted_dt))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_dt))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_dt).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)\n",
    "\n",
    "#get DT depth\n",
    "print(\"DT depth: \", clf_dt.get_depth())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first DT model was trained using default values. The accuracy, while not as high as KNN, was still ok at 81%. The recall for the >50K class did increase to 62% identified, but precision is still pretty low at only 61% correctly predicted. We can also see in the output that this tree had a depth of 51. Maybe trimming this down will help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT Accuracy Score: 0.8097609561752988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.87      0.87      0.87     11360\n",
      "        >50K       0.61      0.61      0.61      3700\n",
      "\n",
      "    accuracy                           0.81     15060\n",
      "   macro avg       0.74      0.74      0.74     15060\n",
      "weighted avg       0.81      0.81      0.81     15060\n",
      "\n",
      "[[9922 1438]\n",
      " [1427 2273]]\n",
      "True Negatives:  9922\n",
      "False Positives:  1438\n",
      "False Negatives:  1427\n",
      "True Positives:  2273\n",
      "DT depth:  40\n"
     ]
    }
   ],
   "source": [
    "#DT model 2\n",
    "#max_depth = 40, criterion = \"entropy\"\n",
    "\n",
    "#build\n",
    "clf_dt = tree.DecisionTreeClassifier(max_depth=40, criterion=\"entropy\", random_state=0)\n",
    "\n",
    "#train\n",
    "clf_dt = clf_dt.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_dt = clf_dt.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"DT Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_dt))\n",
    "print(metrics.classification_report(target_test, target_predicted_dt))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_dt))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_dt).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)\n",
    "\n",
    "#get DT depth\n",
    "print(\"DT depth: \", clf_dt.get_depth())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The splitting criterion was changed for the second model from gini to entropy. The model depth was also reduced to 40, which can help prevent overfitting. This resulted in the same accuracy and precision but a slightly lower recall at 61%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT Accuracy Score: 0.8462815405046481\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.92      0.90     11360\n",
      "        >50K       0.72      0.62      0.66      3700\n",
      "\n",
      "    accuracy                           0.85     15060\n",
      "   macro avg       0.80      0.77      0.78     15060\n",
      "weighted avg       0.84      0.85      0.84     15060\n",
      "\n",
      "[[10457   903]\n",
      " [ 1412  2288]]\n",
      "True Negatives:  10457\n",
      "False Positives:  903\n",
      "False Negatives:  1412\n",
      "True Positives:  2288\n",
      "DT depth:  20\n"
     ]
    }
   ],
   "source": [
    "#DT model 3\n",
    "#max_depth = 20, criterion = \"entropy\", min_samples_split = 50\n",
    "\n",
    "#build\n",
    "clf_dt = tree.DecisionTreeClassifier(max_depth=20, criterion=\"entropy\", min_samples_split=50, random_state=0)\n",
    "\n",
    "#train\n",
    "clf_dt = clf_dt.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_dt = clf_dt.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"DT Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_dt))\n",
    "print(metrics.classification_report(target_test, target_predicted_dt))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_dt))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_dt).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)\n",
    "\n",
    "#get DT depth\n",
    "print(\"DT depth: \", clf_dt.get_depth())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final DT model was trained with criterion of entropy (similar to the 2nd model). The depth was decreased even further from 40 to 20. Finally, the minimum samples required to split was increased from 2 (the default) to 50. The result was a more accurate model (85%), slight higher precision (72%) and the same recall (62%). This model is not great but better than the previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, since this is not a linear/distance based model scaling will not be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.846547144754316\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.92      0.90     11360\n",
      "        >50K       0.72      0.61      0.66      3700\n",
      "\n",
      "    accuracy                           0.85     15060\n",
      "   macro avg       0.80      0.77      0.78     15060\n",
      "weighted avg       0.84      0.85      0.84     15060\n",
      "\n",
      "[[10478   882]\n",
      " [ 1429  2271]]\n",
      "True Negatives:  10478\n",
      "False Positives:  882\n",
      "False Negatives:  1429\n",
      "True Positives:  2271\n"
     ]
    }
   ],
   "source": [
    "#RF model 1 - default parameters\n",
    "\n",
    "#import packages\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#build\n",
    "clf_rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "#train\n",
    "clf_rf = clf_rf.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_rf = clf_rf.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_rf))\n",
    "print(metrics.classification_report(target_test, target_predicted_rf))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_rf))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_rf).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default random forest parameters do pretty well here (when compared to previous models). The results are in-line with the tuned DT model at 85% accuracy. This model identified 61% of the positive class and predicted 72%. Not a great model but one of the better ones so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Depth:  57\n"
     ]
    }
   ],
   "source": [
    "#find avg of max_depth, will be used for tuning\n",
    "#code taken from https://stackoverflow.com/questions/34214087/how-do-you-access-tree-depth-in-pythons-scikit-learn\n",
    "counter = 0\n",
    "running_total = 0\n",
    "for estimator in clf_rf.estimators_:\n",
    "    running_total = running_total + estimator.tree_.max_depth\n",
    "    counter = counter + 1\n",
    "    \n",
    "print('Avg Depth: ', round(running_total/counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90,\n",
       "                                          100, 110, 120, 130, 140, 150, 160,\n",
       "                                          170, 180, 190, 200]},\n",
       "             scoring=make_scorer(recall_score, pos_label=>50K))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RF model - will use grid search to tune this model's n_estimators\n",
    "#some code taken from https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "#will also use modify grid search to use recall score\n",
    "\n",
    "#import packages\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#build\n",
    "clf_rf = RandomForestClassifier(random_state=0)\n",
    "scorer = metrics.make_scorer(metrics.recall_score, pos_label='>50K')\n",
    "\n",
    "#define parameters to test\n",
    "n_estimators = list(np.arange(10, 210, 10))\n",
    "\n",
    "#define param_grid for grid search\n",
    "param_grid = {'n_estimators':n_estimators}\n",
    "\n",
    "#run grid search\n",
    "grid_search = GridSearchCV(clf_rf, param_grid=param_grid,n_jobs=-1,cv=5,scoring=scorer)\n",
    "grid_search.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score 0.6286644234513875\n",
      "Best {'n_estimators': 120}\n"
     ]
    }
   ],
   "source": [
    "#get best grid search parameter\n",
    "print(\"Best Score\", grid_search.best_score_)\n",
    "print(\"Best\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.8461487383798141\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.92      0.90     11360\n",
      "        >50K       0.72      0.61      0.66      3700\n",
      "\n",
      "    accuracy                           0.85     15060\n",
      "   macro avg       0.80      0.77      0.78     15060\n",
      "weighted avg       0.84      0.85      0.84     15060\n",
      "\n",
      "[[10473   887]\n",
      " [ 1430  2270]]\n",
      "True Negatives:  10473\n",
      "False Positives:  887\n",
      "False Negatives:  1430\n",
      "True Positives:  2270\n"
     ]
    }
   ],
   "source": [
    "#RF model 2\n",
    "#n_estimators = 120\n",
    "\n",
    "#build\n",
    "clf_rf = RandomForestClassifier(random_state=0, n_estimators=120)\n",
    "\n",
    "#train\n",
    "clf_rf = clf_rf.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_rf = clf_rf.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_rf))\n",
    "print(metrics.classification_report(target_test, target_predicted_rf))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_rf))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_rf).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search was used for the second model to attempt to tune the number of estimators parameter - this is the number of trees in the forest. The resulting model was about the same as the first RF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.8471447543160691\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.92      0.90     11360\n",
      "        >50K       0.72      0.62      0.66      3700\n",
      "\n",
      "    accuracy                           0.85     15060\n",
      "   macro avg       0.80      0.77      0.78     15060\n",
      "weighted avg       0.84      0.85      0.84     15060\n",
      "\n",
      "[[10480   880]\n",
      " [ 1422  2278]]\n",
      "True Negatives:  10480\n",
      "False Positives:  880\n",
      "False Negatives:  1422\n",
      "True Positives:  2278\n"
     ]
    }
   ],
   "source": [
    "#RF model 3\n",
    "#criterion = \"entropy\"\n",
    "\n",
    "#build\n",
    "clf_rf = RandomForestClassifier(random_state=0, n_estimators=120, criterion=\"entropy\")\n",
    "\n",
    "#train\n",
    "clf_rf = clf_rf.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_rf = clf_rf.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_rf))\n",
    "print(metrics.classification_report(target_test, target_predicted_rf))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_rf))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_rf).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final RF model was switched from gini to entropy splitter criterion. The resulting model had slightly higher accuracy and recall than the first two, but is still about the same as the best DT model. Only 62% of the >50K class was identified and 72% predicted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4: SVM (Linear Kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machine models are distance based, therefore scaling is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.8464807436918991\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.87      0.93      0.90     11360\n",
      "        >50K       0.74      0.58      0.65      3700\n",
      "\n",
      "    accuracy                           0.85     15060\n",
      "   macro avg       0.81      0.76      0.78     15060\n",
      "weighted avg       0.84      0.85      0.84     15060\n",
      "\n",
      "[[10596   764]\n",
      " [ 1548  2152]]\n",
      "True Negatives:  10596\n",
      "False Positives:  764\n",
      "False Negatives:  1548\n",
      "True Positives:  2152\n"
     ]
    }
   ],
   "source": [
    "#SVM_LK model 1 - default parameters\n",
    "\n",
    "#import packages\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#build\n",
    "clf_linearSVC = make_pipeline(StandardScaler(), SVC(kernel='linear'))\n",
    "\n",
    "#train\n",
    "clf_linearSVC = clf_linearSVC.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_linearSVC = clf_linearSVC.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_linearSVC))\n",
    "print(metrics.classification_report(target_test, target_predicted_linearSVC))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_linearSVC))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_linearSVC).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model used default parameter values. The accuracy is pretty good at 85% - similar to other models. The precision is also higher at 74% of the positive class predicted. However, recall is pretty low at only 58% of the positive class identified, resulting in a poor model overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.7887118193891103\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.94      0.77      0.85     11360\n",
      "        >50K       0.54      0.86      0.67      3700\n",
      "\n",
      "    accuracy                           0.79     15060\n",
      "   macro avg       0.74      0.81      0.76     15060\n",
      "weighted avg       0.85      0.79      0.80     15060\n",
      "\n",
      "[[8695 2665]\n",
      " [ 517 3183]]\n",
      "True Negatives:  8695\n",
      "False Positives:  2665\n",
      "False Negatives:  517\n",
      "True Positives:  3183\n"
     ]
    }
   ],
   "source": [
    "#SVM_LK model 2\n",
    "#class_weight = \"balanced\"\n",
    "\n",
    "#build\n",
    "clf_linearSVC = make_pipeline(StandardScaler(), SVC(kernel='linear', class_weight='balanced'))\n",
    "\n",
    "#train\n",
    "clf_linearSVC = clf_linearSVC.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_linearSVC = clf_linearSVC.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_linearSVC))\n",
    "print(metrics.classification_report(target_test, target_predicted_linearSVC))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_linearSVC))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_linearSVC).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since my data is not balanced I decided to change the class_weight parameter to balanced. This will adjust the weights to be better proportioned to the class frequencies. The resulting model actually had worse accuracy (only 79%) and precision (only 54%) but had great recall at 86%. However, due to the low precision this is not a very good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM_LK model - will use grid search to tune this model's C parameter\n",
    "#will also use modify grid search to use recall score\n",
    "\n",
    "#build\n",
    "#clf_linearSVC = make_pipeline(StandardScaler(), SVC(kernel='linear', class_weight='balanced'))\n",
    "#scorer = metrics.make_scorer(metrics.recall_score, pos_label='>50K')\n",
    "\n",
    "#define parameters to test\n",
    "#C = [0.1, 0.5]\n",
    "\n",
    "#define param_grid for grid search\n",
    "#param_grid = {'C':C}\n",
    "\n",
    "#run grid search\n",
    "#note - due to how long this model takes to run, cv reduced from 5 to 3\n",
    "#n_jobs = -2 uses all but 1 CPU\n",
    "#grid_search = GridSearchCV(clf_linearSVC['svc'], param_grid=param_grid,n_jobs=-2,cv=3,scoring=scorer)\n",
    "#grid_search.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output from above:\n",
    "(this grid search took 2+ hours so it was not re-run)\n",
    "\n",
    "GridSearchCV(cv=3, estimator=SVC(class_weight='balanced', kernel='linear'),\n",
    "             n_jobs=-2, param_grid={'C': [0.1, 0.5]},\n",
    "             scoring=make_scorer(recall_score, pos_label=>50K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get best grid search parameter\n",
    "#print(\"Scores\", grid_search.cv_results_)\n",
    "#print(\"Best Score\", grid_search.best_score_)\n",
    "#print(\"Best\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from above:\n",
    "(this grid search took 2+ hours so it was not re-run)\n",
    "\n",
    "Scores {'mean_fit_time': array([3953.03323118, 4381.72333439]), 'std_fit_time': array([427.8688763 , 476.00041536]), 'mean_score_time': array([23.28372049, 19.40571936]), 'std_score_time': array([2.13467588, 3.76091124]), 'param_C': masked_array(data=[0.1, 0.5],\n",
    "             mask=[False, False],\n",
    "       fill_value='?',\n",
    "            dtype=object), 'params': [{'C': 0.1}, {'C': 0.5}], 'split0_test_score': array([0.70943245, 0.69344524]), 'split1_test_score': array([0.73871354, 0.75029964]), 'split2_test_score': array([0.73351978, 0.747503  ]), 'mean_test_score': array([0.72722192, 0.73041596]), 'std_test_score': array([0.01275651, 0.02616716]), 'rank_test_score': array([2, 1])}\n",
    "Best Score 0.7304159602135843\n",
    "Best {'C': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.7887118193891103\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.94      0.77      0.85     11360\n",
      "        >50K       0.54      0.86      0.67      3700\n",
      "\n",
      "    accuracy                           0.79     15060\n",
      "   macro avg       0.74      0.81      0.76     15060\n",
      "weighted avg       0.85      0.79      0.80     15060\n",
      "\n",
      "[[8696 2664]\n",
      " [ 518 3182]]\n",
      "True Negatives:  8696\n",
      "False Positives:  2664\n",
      "False Negatives:  518\n",
      "True Positives:  3182\n"
     ]
    }
   ],
   "source": [
    "#SVM_LK model 3\n",
    "#class_weight = \"balanced\", C = 0.5\n",
    "\n",
    "#build\n",
    "clf_linearSVC = make_pipeline(StandardScaler(), SVC(kernel='linear', class_weight='balanced', C=0.5))\n",
    "\n",
    "#train\n",
    "clf_linearSVC = clf_linearSVC.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_linearSVC = clf_linearSVC.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_linearSVC))\n",
    "print(metrics.classification_report(target_test, target_predicted_linearSVC))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_linearSVC))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_linearSVC).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search was used to attempt to tune the C parameter for the final SVM model. As noted above this process took quite a while to complete which is why more values were not tested. This parameter adjusts the soft margin used in the model which defines the support vectors created (and how the data is trained). In this case it was shown that 0.5 was the optimal value (default = 1). The resulting model, however, was not any different in terms of predictive power than the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 5: SVM (RBF Kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs require scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.799601593625498\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.94      0.79      0.86     11360\n",
      "        >50K       0.56      0.84      0.67      3700\n",
      "\n",
      "    accuracy                           0.80     15060\n",
      "   macro avg       0.75      0.81      0.76     15060\n",
      "weighted avg       0.84      0.80      0.81     15060\n",
      "\n",
      "[[8952 2408]\n",
      " [ 610 3090]]\n",
      "True Negatives:  8952\n",
      "False Positives:  2408\n",
      "False Negatives:  610\n",
      "True Positives:  3090\n"
     ]
    }
   ],
   "source": [
    "#SVM_RK model 1\n",
    "#class_weight = \"balanced\", C = 2\n",
    "\n",
    "#build\n",
    "clf_rbkSVC = make_pipeline(StandardScaler(), SVC(kernel='rbf', class_weight='balanced', C=2))\n",
    "\n",
    "#train\n",
    "clf_rbkSVC = clf_rbkSVC.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_rbkSVC = clf_rbkSVC.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_rbkSVC))\n",
    "print(metrics.classification_report(target_test, target_predicted_rbkSVC))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_rbkSVC))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_rbkSVC).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was set up similar to the linear SVM models. A C value of 2 was used and class_weight set to balanced. The resulting model was slightly more accurate than linear SVM at 80%. However, precision did not change and recall dropped from 86% to 84%. This recall value is still pretty good but when paired with the low precision this is not a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.7990039840637451\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.94      0.79      0.86     11360\n",
      "        >50K       0.56      0.84      0.67      3700\n",
      "\n",
      "    accuracy                           0.80     15060\n",
      "   macro avg       0.75      0.81      0.76     15060\n",
      "weighted avg       0.84      0.80      0.81     15060\n",
      "\n",
      "[[8943 2417]\n",
      " [ 610 3090]]\n",
      "True Negatives:  8943\n",
      "False Positives:  2417\n",
      "False Negatives:  610\n",
      "True Positives:  3090\n"
     ]
    }
   ],
   "source": [
    "#SVM_RK model 2\n",
    "#gamma = 0.01\n",
    "#NOTE: also tried gamma = 1 and gamma = 0.1, this model was the best\n",
    "\n",
    "#build\n",
    "clf_rbkSVC = make_pipeline(StandardScaler(), SVC(kernel='rbf', class_weight='balanced', C=2, gamma=0.01))\n",
    "\n",
    "#train\n",
    "clf_rbkSVC = clf_rbkSVC.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_rbkSVC = clf_rbkSVC.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_rbkSVC))\n",
    "print(metrics.classification_report(target_test, target_predicted_rbkSVC))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_rbkSVC))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_rbkSVC).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gamma parameter was tuned in the second radial kernel model. This value is used for non-linear hyperplanes. It was determined that gamma = 0.01 was the optimal value. Due to the low value overfitting should not be much of a concern. The resulting model was not any better than the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.7875166002656042\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.94      0.76      0.84     11360\n",
      "        >50K       0.54      0.86      0.66      3700\n",
      "\n",
      "    accuracy                           0.79     15060\n",
      "   macro avg       0.74      0.81      0.75     15060\n",
      "weighted avg       0.84      0.79      0.80     15060\n",
      "\n",
      "[[8687 2673]\n",
      " [ 527 3173]]\n",
      "True Negatives:  8687\n",
      "False Positives:  2673\n",
      "False Negatives:  527\n",
      "True Positives:  3173\n"
     ]
    }
   ],
   "source": [
    "#SVM_RK model 3\n",
    "#gamma = 0.001\n",
    "\n",
    "#build\n",
    "clf_rbkSVC = make_pipeline(StandardScaler(), SVC(kernel='rbf', class_weight='balanced', C=2, gamma=0.001))\n",
    "\n",
    "#train\n",
    "clf_rbkSVC = clf_rbkSVC.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_rbkSVC = clf_rbkSVC.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_rbkSVC))\n",
    "print(metrics.classification_report(target_test, target_predicted_rbkSVC))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_rbkSVC))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_rbkSVC).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third model used an even further reduced gamma value. Similar to before, this model showed no improvement. Overall, this kernel did not produce very good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 6: Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD is also linear-based, scaling will be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.8382470119521912\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.87      0.92      0.90     11360\n",
      "        >50K       0.70      0.60      0.64      3700\n",
      "\n",
      "    accuracy                           0.84     15060\n",
      "   macro avg       0.79      0.76      0.77     15060\n",
      "weighted avg       0.83      0.84      0.83     15060\n",
      "\n",
      "[[10421   939]\n",
      " [ 1497  2203]]\n",
      "True Negatives:  10421\n",
      "False Positives:  939\n",
      "False Negatives:  1497\n",
      "True Positives:  2203\n"
     ]
    }
   ],
   "source": [
    "#SGD model 1 - default parameters\n",
    "\n",
    "#import packages\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "#build\n",
    "clf_SGD = make_pipeline(StandardScaler(), SGDClassifier(random_state=0))\n",
    "\n",
    "#train\n",
    "clf_SGD = clf_SGD.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_SGD = clf_SGD.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_SGD))\n",
    "print(metrics.classification_report(target_test, target_predicted_SGD))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_SGD))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_SGD).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first SGD model was built using default parameters. This model had 84% accuracy. The positive class (>50K) was identified correctly 60% of the time and correctly predicted 70% of the time. This model is not the best, but is in-line with some of the DT and RF models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.8426958831341301\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.92      0.90     11360\n",
      "        >50K       0.71      0.60      0.65      3700\n",
      "\n",
      "    accuracy                           0.84     15060\n",
      "   macro avg       0.79      0.76      0.78     15060\n",
      "weighted avg       0.84      0.84      0.84     15060\n",
      "\n",
      "[[10465   895]\n",
      " [ 1474  2226]]\n",
      "True Negatives:  10465\n",
      "False Positives:  895\n",
      "False Negatives:  1474\n",
      "True Positives:  2226\n"
     ]
    }
   ],
   "source": [
    "#SGD model 2\n",
    "#loss = \"log\"\n",
    "#NOTE: also tried loss = \"modified_huber\" with and without class_weight = \"balanced\"\n",
    "\n",
    "#build\n",
    "clf_SGD = make_pipeline(StandardScaler(), SGDClassifier(random_state=0, loss=\"log\"))\n",
    "\n",
    "#train\n",
    "clf_SGD = clf_SGD.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_SGD = clf_SGD.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_SGD))\n",
    "print(metrics.classification_report(target_test, target_predicted_SGD))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_SGD))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_SGD).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different loss functions were attempted in the second model. The log loss function (equivalent to logistic regression) ended up producing a model with slightly better accuracy and precision than the first. However, this model is still not great. Precision only increased by 1% and recall did not improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.7980079681274901\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.94      0.79      0.85     11360\n",
      "        >50K       0.56      0.84      0.67      3700\n",
      "\n",
      "    accuracy                           0.80     15060\n",
      "   macro avg       0.75      0.81      0.76     15060\n",
      "weighted avg       0.84      0.80      0.81     15060\n",
      "\n",
      "[[8923 2437]\n",
      " [ 605 3095]]\n",
      "True Negatives:  8923\n",
      "False Positives:  2437\n",
      "False Negatives:  605\n",
      "True Positives:  3095\n"
     ]
    }
   ],
   "source": [
    "#SGD model 3\n",
    "#loss = \"log\", class_weight = \"balanced\"\n",
    "\n",
    "#build\n",
    "clf_SGD = make_pipeline(StandardScaler(), SGDClassifier(random_state=0, class_weight=\"balanced\", loss=\"log\"))\n",
    "\n",
    "#train\n",
    "clf_SGD = clf_SGD.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_SGD = clf_SGD.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_SGD))\n",
    "print(metrics.classification_report(target_test, target_predicted_SGD))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_SGD))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_SGD).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final model class_weight of balanced was used to change the class weights. This model, similar to the SVM models, had slightly lower accuracy (only 80%) but an increased recall of 84%. Also similar to the SVM models, the precision was too low (56%) to be considered good. These models seem to have a hard time balancing precision and recall for the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Adaboost (with 2 learners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using the random forest classifier with adaboost, scaling is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.8573705179282869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.94      0.91     11360\n",
      "        >50K       0.77      0.60      0.67      3700\n",
      "\n",
      "    accuracy                           0.86     15060\n",
      "   macro avg       0.82      0.77      0.79     15060\n",
      "weighted avg       0.85      0.86      0.85     15060\n",
      "\n",
      "[[10699   661]\n",
      " [ 1487  2213]]\n",
      "True Negatives:  10699\n",
      "False Positives:  661\n",
      "False Negatives:  1487\n",
      "True Positives:  2213\n"
     ]
    }
   ],
   "source": [
    "#Adaboost model 1 - default parameters\n",
    "\n",
    "#import packages\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#build\n",
    "clf_ADA = AdaBoostClassifier(RandomForestClassifier(max_depth=1, criterion=\"entropy\"), random_state=0)\n",
    "\n",
    "#train\n",
    "clf_ADA = clf_ADA.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_ADA = clf_ADA.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_ADA))\n",
    "print(metrics.classification_report(target_test, target_predicted_ADA))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_ADA))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_ADA).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest was chosen for the base estimator due to it having the best results so far. This model was trained with a max depth of 1. This max depth is used due to the requirement of having weak learners. Criterion of entropy was selected due to it showing slight improvement over gini in the original DT/RF models.\n",
    "\n",
    "The resulting model was one of the most accurate so far at 86%. This model also correctly identifed the positive class 60% of the time and predicted correctly 77% of the time. While the recall is a little lower than the DT/RF models, the accuracy and precision are turning in the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.8632138114209827\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.89      0.93      0.91     11360\n",
      "        >50K       0.76      0.65      0.70      3700\n",
      "\n",
      "    accuracy                           0.86     15060\n",
      "   macro avg       0.83      0.79      0.81     15060\n",
      "weighted avg       0.86      0.86      0.86     15060\n",
      "\n",
      "[[10611   749]\n",
      " [ 1311  2389]]\n",
      "True Negatives:  10611\n",
      "False Positives:  749\n",
      "False Negatives:  1311\n",
      "True Positives:  2389\n"
     ]
    }
   ],
   "source": [
    "#Adaboost model 2\n",
    "#max_depth = 10\n",
    "\n",
    "#build\n",
    "clf_ADA = AdaBoostClassifier(RandomForestClassifier(criterion=\"entropy\", max_depth=10), random_state=0)\n",
    "\n",
    "#train\n",
    "clf_ADA = clf_ADA.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_ADA = clf_ADA.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_ADA))\n",
    "print(metrics.classification_report(target_test, target_predicted_ADA))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_ADA))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_ADA).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max_depth was increased in this model to 10. The resulting model has improved recall over the previous at 65%. The precision did drop 1% but this is pretty negligible. While this is probably one of the better models generated, I'm still not sure if it is a great model. Since the classes are not balanced it would be nice to get a higher recall for the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.8662682602921646\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.89      0.94      0.91     11360\n",
      "        >50K       0.77      0.65      0.70      3700\n",
      "\n",
      "    accuracy                           0.87     15060\n",
      "   macro avg       0.83      0.79      0.81     15060\n",
      "weighted avg       0.86      0.87      0.86     15060\n",
      "\n",
      "[[10642   718]\n",
      " [ 1296  2404]]\n",
      "True Negatives:  10642\n",
      "False Positives:  718\n",
      "False Negatives:  1296\n",
      "True Positives:  2404\n"
     ]
    }
   ],
   "source": [
    "#Adaboost model 3\n",
    "#learning_rate = 0.5\n",
    "#NOTE: also tried learning_rate = 0.1 and learning_rate = 0.01\n",
    "\n",
    "#build\n",
    "clf_ADA = AdaBoostClassifier(RandomForestClassifier(criterion=\"entropy\", max_depth=10), random_state=0, learning_rate=0.5)\n",
    "\n",
    "#train\n",
    "clf_ADA = clf_ADA.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_ADA = clf_ADA.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_ADA))\n",
    "print(metrics.classification_report(target_test, target_predicted_ADA))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_ADA))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_ADA).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rates were adjusted for this model. The learning rate determines the weight applied to each classifier during boosting. A rate of 0.5 was found to have the highest accuracy and increased the precision back up to 77%. Recall, however, did not improve from 65%. Again, this model is one of the better ones so far but not necessarily great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Bagging Classifier (with at least 1 learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the default base classifier is a decision tree scaling is not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.8443559096945551\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.87      0.93      0.90     11360\n",
      "        >50K       0.73      0.58      0.65      3700\n",
      "\n",
      "    accuracy                           0.84     15060\n",
      "   macro avg       0.80      0.76      0.77     15060\n",
      "weighted avg       0.84      0.84      0.84     15060\n",
      "\n",
      "[[10552   808]\n",
      " [ 1536  2164]]\n",
      "True Negatives:  10552\n",
      "False Positives:  808\n",
      "False Negatives:  1536\n",
      "True Positives:  2164\n"
     ]
    }
   ],
   "source": [
    "#Bagging model 1 - default parameters\n",
    "\n",
    "#import packages\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#build\n",
    "clf_BAG = BaggingClassifier(random_state=0)\n",
    "\n",
    "#train\n",
    "clf_BAG = clf_BAG.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_BAG = clf_BAG.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_BAG))\n",
    "print(metrics.classification_report(target_test, target_predicted_BAG))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_BAG))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_BAG).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with most of the other models, this one was first built using default values. This model has good accuracy but the recall is significantly less than some of the better models at 58%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.8482735723771581\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.92      0.90     11360\n",
      "        >50K       0.73      0.62      0.67      3700\n",
      "\n",
      "    accuracy                           0.85     15060\n",
      "   macro avg       0.80      0.77      0.78     15060\n",
      "weighted avg       0.84      0.85      0.84     15060\n",
      "\n",
      "[[10497   863]\n",
      " [ 1422  2278]]\n",
      "True Negatives:  10497\n",
      "False Positives:  863\n",
      "False Negatives:  1422\n",
      "True Positives:  2278\n"
     ]
    }
   ],
   "source": [
    "#Bagging model 2\n",
    "#n_estimators = 200\n",
    "#NOTE: also tried n_estimators = 50 and n_estimators = 100\n",
    "\n",
    "#build\n",
    "clf_BAG = BaggingClassifier(random_state=0, n_estimators=200)\n",
    "\n",
    "#train\n",
    "clf_BAG = clf_BAG.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_BAG = clf_BAG.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_BAG))\n",
    "print(metrics.classification_report(target_test, target_predicted_BAG))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_BAG))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_BAG).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of estimators (or number of trees) was tuned in the second model. The default n_estimators value is 10. It was determined that 200 estimators provided a better model with 85% accuracy. The positive class was correctly identified 62% of the time and correctly predicted 73% of the time. This model is not quite as good as the AdaBoost models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.852722443559097\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.93      0.90     11360\n",
      "        >50K       0.74      0.62      0.67      3700\n",
      "\n",
      "    accuracy                           0.85     15060\n",
      "   macro avg       0.81      0.77      0.79     15060\n",
      "weighted avg       0.85      0.85      0.85     15060\n",
      "\n",
      "[[10547   813]\n",
      " [ 1405  2295]]\n",
      "True Negatives:  10547\n",
      "False Positives:  813\n",
      "False Negatives:  1405\n",
      "True Positives:  2295\n"
     ]
    }
   ],
   "source": [
    "#Bagging model 3\n",
    "#base_estimator = RandomForestClassifier, n_estimators=200\n",
    "\n",
    "#build\n",
    "clf_BAG = BaggingClassifier(base_estimator=RandomForestClassifier(criterion=\"entropy\"), random_state=0, n_estimators=200)\n",
    "\n",
    "#train\n",
    "clf_BAG = clf_BAG.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_BAG = clf_BAG.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_BAG))\n",
    "print(metrics.classification_report(target_test, target_predicted_BAG))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_BAG))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_BAG).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final model the base_estimator was changed to Random Forest since we had some success earlier. The number of estimators did not change from model 2. This model is slightly better than the DT model in terms of the metrics but still not as good as AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting does not require scaling since it uses trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.8664010624169987\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.95      0.91     11360\n",
      "        >50K       0.79      0.62      0.70      3700\n",
      "\n",
      "    accuracy                           0.87     15060\n",
      "   macro avg       0.84      0.78      0.80     15060\n",
      "weighted avg       0.86      0.87      0.86     15060\n",
      "\n",
      "[[10755   605]\n",
      " [ 1407  2293]]\n",
      "True Negatives:  10755\n",
      "False Positives:  605\n",
      "False Negatives:  1407\n",
      "True Positives:  2293\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting model 1 - default parameters\n",
    "\n",
    "#import packages\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#build\n",
    "clf_GBC = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "#train\n",
    "clf_GBC = clf_GBC.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_GBC = clf_GBC.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_GBC))\n",
    "print(metrics.classification_report(target_test, target_predicted_GBC))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_GBC))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_GBC).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model, ran with defaults, resulted in a fairly accurate and precise model. At 79% precision, this is the highest so far. The recall is very average though, only at 62%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.8702523240371846\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.90      0.94      0.92     11360\n",
      "        >50K       0.77      0.67      0.72      3700\n",
      "\n",
      "    accuracy                           0.87     15060\n",
      "   macro avg       0.83      0.80      0.82     15060\n",
      "weighted avg       0.87      0.87      0.87     15060\n",
      "\n",
      "[[10639   721]\n",
      " [ 1233  2467]]\n",
      "True Negatives:  10639\n",
      "False Positives:  721\n",
      "False Negatives:  1233\n",
      "True Positives:  2467\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting model 2\n",
    "#n_estimators = 1000\n",
    "\n",
    "#build\n",
    "clf_GBC = GradientBoostingClassifier(random_state=0, n_estimators=1000)\n",
    "\n",
    "#train\n",
    "clf_GBC = clf_GBC.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_GBC = clf_GBC.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_GBC))\n",
    "print(metrics.classification_report(target_test, target_predicted_GBC))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_GBC))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_GBC).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of trees (n_estimators) was tuned for the second model. The docs for this algorithm state that gradient boosting is fairly robust to overfitting so large numbers of trees tend to perform pretty well. In this stage the number of trees was increased 10-fold from 100 to 1000. The resulting model has the highest accuracy and recall seen so far at 87% and 67%, respectively. While the highest recall yet, this model may still not be considered \"great\".\n",
    "\n",
    "Disclaimer: some of the SVM models actually had higher recall but the precision was VERY poor and therefore not considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.8712483399734395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.89      0.94      0.92     11360\n",
      "        >50K       0.78      0.66      0.72      3700\n",
      "\n",
      "    accuracy                           0.87     15060\n",
      "   macro avg       0.84      0.80      0.82     15060\n",
      "weighted avg       0.87      0.87      0.87     15060\n",
      "\n",
      "[[10685   675]\n",
      " [ 1264  2436]]\n",
      "True Negatives:  10685\n",
      "False Positives:  675\n",
      "False Negatives:  1264\n",
      "True Positives:  2436\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting model 3\n",
    "#n_estimators = 1000, learning_rate = 0.05\n",
    "#NOTE: also tried learning_rate = 0.01\n",
    "\n",
    "#build\n",
    "clf_GBC = GradientBoostingClassifier(random_state=0, n_estimators=1000, learning_rate=0.05)\n",
    "\n",
    "#train\n",
    "clf_GBC = clf_GBC.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_GBC = clf_GBC.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_GBC))\n",
    "print(metrics.classification_report(target_test, target_predicted_GBC))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_GBC))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_GBC).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate was adjusted for the final model. The docs state that this parameter has a trade off with the number of trees. The resulting model was about the same as the previous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No scaling is required for Extra Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.8283532536520585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.87      0.90      0.89     11360\n",
      "        >50K       0.67      0.60      0.63      3700\n",
      "\n",
      "    accuracy                           0.83     15060\n",
      "   macro avg       0.77      0.75      0.76     15060\n",
      "weighted avg       0.82      0.83      0.83     15060\n",
      "\n",
      "[[10255  1105]\n",
      " [ 1480  2220]]\n",
      "True Negatives:  10255\n",
      "False Positives:  1105\n",
      "False Negatives:  1480\n",
      "True Positives:  2220\n"
     ]
    }
   ],
   "source": [
    "#Extra Trees model 1 - default parameters\n",
    "\n",
    "#import packages\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "#build\n",
    "clf_ext = ExtraTreesClassifier(random_state=0, n_jobs=-2)\n",
    "\n",
    "#train\n",
    "clf_ext = clf_ext.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_ext = clf_ext.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_ext))\n",
    "print(metrics.classification_report(target_test, target_predicted_ext))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_ext))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_ext).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra trees did not perform as well as some of the other tests. It only correctly identified 60% of the positive class (>50K) and of those only 67% were correctly predicted. This model is poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.8260956175298805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.87      0.90      0.89     11360\n",
      "        >50K       0.66      0.59      0.63      3700\n",
      "\n",
      "    accuracy                           0.83     15060\n",
      "   macro avg       0.77      0.75      0.76     15060\n",
      "weighted avg       0.82      0.83      0.82     15060\n",
      "\n",
      "[[10258  1102]\n",
      " [ 1517  2183]]\n",
      "True Negatives:  10258\n",
      "False Positives:  1102\n",
      "False Negatives:  1517\n",
      "True Positives:  2183\n"
     ]
    }
   ],
   "source": [
    "#Extra Trees model 2\n",
    "#class_weight = \"balanced\"\n",
    "#NOTE: also tried criterion = \"entropy\"\n",
    "\n",
    "#build\n",
    "clf_ext = ExtraTreesClassifier(random_state=0, n_jobs=-2, class_weight=\"balanced\")\n",
    "\n",
    "#train\n",
    "clf_ext = clf_ext.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_ext = clf_ext.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_ext))\n",
    "print(metrics.classification_report(target_test, target_predicted_ext))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_ext))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_ext).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A balanced class weight was used for this model. For this case balance the weights did not help and resulted in a worse model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.8291500664010624\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.87      0.90      0.89     11360\n",
      "        >50K       0.67      0.60      0.63      3700\n",
      "\n",
      "    accuracy                           0.83     15060\n",
      "   macro avg       0.77      0.75      0.76     15060\n",
      "weighted avg       0.82      0.83      0.83     15060\n",
      "\n",
      "[[10263  1097]\n",
      " [ 1476  2224]]\n",
      "True Negatives:  10263\n",
      "False Positives:  1097\n",
      "False Negatives:  1476\n",
      "True Positives:  2224\n"
     ]
    }
   ],
   "source": [
    "#Extra Trees model 3\n",
    "#max_depth = 50\n",
    "#NOTE: also tried max_depth = 5, 10, 15, 35, 60\n",
    "\n",
    "#build\n",
    "clf_ext = ExtraTreesClassifier(random_state=0, n_jobs=-2, max_depth=50)\n",
    "\n",
    "#train\n",
    "clf_ext = clf_ext.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_ext = clf_ext.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_ext))\n",
    "print(metrics.classification_report(target_test, target_predicted_ext))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_ext))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_ext).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max_depth was parameter was tuned for the final model since this parameter helped with previous tree models. Multiple values were tried but max_depth of 50 seemed to have the best results. This model has metrics consistent with the previous two models and is not very good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. ANN (with different hidden layers and nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks do require scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.848937583001328\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.93      0.90     11360\n",
      "        >50K       0.73      0.61      0.67      3700\n",
      "\n",
      "    accuracy                           0.85     15060\n",
      "   macro avg       0.80      0.77      0.78     15060\n",
      "weighted avg       0.84      0.85      0.84     15060\n",
      "\n",
      "[[10520   840]\n",
      " [ 1435  2265]]\n",
      "True Negatives:  10520\n",
      "False Positives:  840\n",
      "False Negatives:  1435\n",
      "True Positives:  2265\n"
     ]
    }
   ],
   "source": [
    "#ANN model 1\n",
    "#hidden_layer_sizes = (5)\n",
    "#hidden_layer_sizes format (x, y) where x = number of nodes in a hidden layer and y = number of hidden layers (1 if omitted)\n",
    "#NOTE: max_iter increased due to convergence error\n",
    "\n",
    "#import packages\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#build\n",
    "clf_ann = make_pipeline(StandardScaler(), MLPClassifier(random_state=0, max_iter=500, hidden_layer_sizes=5))\n",
    "\n",
    "#train\n",
    "clf_ann = clf_ann.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_ann = clf_ann.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_ann))\n",
    "print(metrics.classification_report(target_test, target_predicted_ann))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_ann))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_ann).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was built using a hidden layer with 5 nodes. The model has 85% accuracy. 67% of the >50K class was correctly identified and 73% of those were correctly predicted. This model is not one of the best but is fairly consistent with the other \"average\" ranked models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.850597609561753\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.89      0.92      0.90     11360\n",
      "        >50K       0.72      0.64      0.68      3700\n",
      "\n",
      "    accuracy                           0.85     15060\n",
      "   macro avg       0.80      0.78      0.79     15060\n",
      "weighted avg       0.85      0.85      0.85     15060\n",
      "\n",
      "[[10435   925]\n",
      " [ 1325  2375]]\n",
      "True Negatives:  10435\n",
      "False Positives:  925\n",
      "False Negatives:  1325\n",
      "True Positives:  2375\n"
     ]
    }
   ],
   "source": [
    "#ANN model 2\n",
    "#hidden_layer_sizes=(5,2)\n",
    "\n",
    "#build\n",
    "clf_ann = make_pipeline(StandardScaler(), MLPClassifier(random_state=0, max_iter=500, hidden_layer_sizes=(5,2)))\n",
    "\n",
    "#train\n",
    "clf_ann = clf_ann.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_ann = clf_ann.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_ann))\n",
    "print(metrics.classification_report(target_test, target_predicted_ann))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_ann))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_ann).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional hidden layer of 5 nodes was added to the second model. The resulting model saw increased recall (64%) with minimal precision trade-off (reduced from 73% to 72%). This is one of the higher recall values but AdaBoost preformed a litte better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 0.8422974767596282\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.91      0.90     11360\n",
      "        >50K       0.70      0.62      0.66      3700\n",
      "\n",
      "    accuracy                           0.84     15060\n",
      "   macro avg       0.79      0.77      0.78     15060\n",
      "weighted avg       0.84      0.84      0.84     15060\n",
      "\n",
      "[[10377   983]\n",
      " [ 1392  2308]]\n",
      "True Negatives:  10377\n",
      "False Positives:  983\n",
      "False Negatives:  1392\n",
      "True Positives:  2308\n"
     ]
    }
   ],
   "source": [
    "#ANN model 3\n",
    "#hidden_layer_sizes = (20)\n",
    "#NOTE: also tried hidden_layer_sizes = (10), (5,3)\n",
    "\n",
    "#build\n",
    "clf_ann = make_pipeline(StandardScaler(), MLPClassifier(random_state=0, max_iter=500, hidden_layer_sizes=(20)))\n",
    "\n",
    "#train\n",
    "clf_ann = clf_ann.fit(features_train, target_train)\n",
    "\n",
    "#validate\n",
    "target_predicted_ann = clf_ann.predict(features_test)\n",
    "\n",
    "#scores\n",
    "print(\"RF Accuracy Score:\", metrics.accuracy_score(target_test, target_predicted_ann))\n",
    "print(metrics.classification_report(target_test, target_predicted_ann))\n",
    "print(metrics.confusion_matrix(target_test, target_predicted_ann))\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(target_test, target_predicted_ann).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional hidden layer sizes were tuned for the final model. The end result was not as good as the previous model using 2 5-node hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking itself does not require scaling but the individual learners may, depending on which model is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81 (+/- 0.01) [Logistic Regression]\n",
      "Precision: 0.58 (+/- 0.01) [Logistic Regression]\n",
      "Recall: 0.84 (+/- 0.01) [Logistic Regression]\n",
      "Accuracy: 0.79 (+/- 0.00) [Naive Bayes]\n",
      "Precision: 0.66 (+/- 0.02) [Naive Bayes]\n",
      "Recall: 0.31 (+/- 0.01) [Naive Bayes]\n",
      "Accuracy: 0.85 (+/- 0.00) [Random Forest]\n",
      "Precision: 0.74 (+/- 0.01) [Random Forest]\n",
      "Recall: 0.62 (+/- 0.01) [Random Forest]\n",
      "Accuracy: 0.83 (+/- 0.01) [AdaBoost Decision Tree]\n",
      "Precision: 0.66 (+/- 0.03) [AdaBoost Decision Tree]\n",
      "Recall: 0.62 (+/- 0.01) [AdaBoost Decision Tree]\n",
      "Accuracy: 0.85 (+/- 0.00) [Second Stage Learner]\n",
      "Precision: 0.78 (+/- 0.02) [Second Stage Learner]\n",
      "Recall: 0.54 (+/- 0.02) [Second Stage Learner]\n"
     ]
    }
   ],
   "source": [
    "#Stacking model 1 - default values from Module 4 example notebook\n",
    "#NOTE: max_iter increased to 500 for learner_1 due to convergence error\n",
    "\n",
    "#import packages\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#hitting pos_label error -> need to transform the target_train set into binary\n",
    "#some code taken from https://stackoverflow.com/questions/27357121/scikit-calculate-precision-and-recall-using-cross-val-score-function\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "ttb = lb.fit_transform(target_train).ravel()\n",
    "\n",
    "#build/train/validate\n",
    "learner_1 = make_pipeline(StandardScaler(), LogisticRegression(class_weight='balanced', max_iter=500))\n",
    "learner_2 = GaussianNB()\n",
    "learner_3 = RandomForestClassifier(class_weight='balanced', max_features='auto', n_estimators=100)\n",
    "learner_4 = AdaBoostClassifier(tree.DecisionTreeClassifier(class_weight='balanced'),\n",
    "                         algorithm=\"SAMME.R\",n_estimators=100)\n",
    "stacked_learner = VotingClassifier(estimators=[('lr', learner_1), ('nb', learner_2),\n",
    "                                              ('rf', learner_3), ('adaboost', learner_4)], voting='hard', n_jobs=-2)\n",
    "for MV, label in zip([learner_1, learner_2, learner_3, learner_4, stacked_learner], \n",
    "                     ['Logistic Regression', 'Naive Bayes', 'Random Forest', 'AdaBoost Decision Tree', 'Second Stage Learner']):\n",
    "    scores_a = cross_val_score(MV, features_train, ttb, cv=5, scoring='accuracy')\n",
    "    scores_p = cross_val_score(MV, features_train, ttb, cv=5, scoring='precision')\n",
    "    scores_r = cross_val_score(MV, features_train, ttb, cv=5, scoring='recall')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores_a.mean(), scores_a.std(), label))\n",
    "    print(\"Precision: %0.2f (+/- %0.2f) [%s]\" % (scores_p.mean(), scores_p.std(), label))\n",
    "    print(\"Recall: %0.2f (+/- %0.2f) [%s]\" % (scores_r.mean(), scores_r.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was built using defaults from the example in the course material. The individual learners used were logistic regression, naive bayes, random forest, and adaboost (using decision trees). This model performed pretty poorly overall with a recall of only 54%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85 (+/- 0.00) [Logistic Regression]\n",
      "Precision: 0.74 (+/- 0.01) [Logistic Regression]\n",
      "Recall: 0.61 (+/- 0.01) [Logistic Regression]\n",
      "Accuracy: 0.79 (+/- 0.00) [Naive Bayes]\n",
      "Precision: 0.66 (+/- 0.02) [Naive Bayes]\n",
      "Recall: 0.31 (+/- 0.01) [Naive Bayes]\n",
      "Accuracy: 0.85 (+/- 0.00) [Random Forest]\n",
      "Precision: 0.73 (+/- 0.01) [Random Forest]\n",
      "Recall: 0.63 (+/- 0.01) [Random Forest]\n",
      "Accuracy: 0.82 (+/- 0.01) [AdaBoost Decision Tree]\n",
      "Precision: 0.64 (+/- 0.02) [AdaBoost Decision Tree]\n",
      "Recall: 0.62 (+/- 0.01) [AdaBoost Decision Tree]\n",
      "Accuracy: 0.85 (+/- 0.00) [Second Stage Learner]\n",
      "Precision: 0.82 (+/- 0.01) [Second Stage Learner]\n",
      "Recall: 0.49 (+/- 0.02) [Second Stage Learner]\n"
     ]
    }
   ],
   "source": [
    "#Stacking model 2\n",
    "#removed class_weight=\"balanced\"\n",
    "\n",
    "#build/train/validate\n",
    "learner_1 = make_pipeline(StandardScaler(), LogisticRegression(max_iter=500))\n",
    "learner_2 = GaussianNB()\n",
    "learner_3 = RandomForestClassifier(max_features='auto', n_estimators=100)\n",
    "learner_4 = AdaBoostClassifier(tree.DecisionTreeClassifier(),\n",
    "                         algorithm=\"SAMME.R\",n_estimators=100)\n",
    "stacked_learner = VotingClassifier(estimators=[('lr', learner_1), ('nb', learner_2),\n",
    "                                              ('rf', learner_3), ('adaboost', learner_4)], voting='hard', n_jobs=-2)\n",
    "for MV, label in zip([learner_1, learner_2, learner_3, learner_4, stacked_learner], \n",
    "                     ['Logistic Regression', 'Naive Bayes', 'Random Forest', 'AdaBoost Decision Tree', 'Second Stage Learner']):\n",
    "    scores_a = cross_val_score(MV, features_train, ttb, cv=5, scoring='accuracy')\n",
    "    scores_p = cross_val_score(MV, features_train, ttb, cv=5, scoring='precision')\n",
    "    scores_r = cross_val_score(MV, features_train, ttb, cv=5, scoring='recall')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores_a.mean(), scores_a.std(), label))\n",
    "    print(\"Precision: %0.2f (+/- %0.2f) [%s]\" % (scores_p.mean(), scores_p.std(), label))\n",
    "    print(\"Recall: %0.2f (+/- %0.2f) [%s]\" % (scores_r.mean(), scores_r.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual learners were kept the same for the second model but the class_weight of balanced was removed. This model performed worse than the previous with only 49% recall. This is the worst so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85 (+/- 0.00) [Logistic Regression]\n",
      "Precision: 0.74 (+/- 0.01) [Logistic Regression]\n",
      "Recall: 0.61 (+/- 0.01) [Logistic Regression]\n",
      "Accuracy: 0.79 (+/- 0.00) [Naive Bayes]\n",
      "Precision: 0.66 (+/- 0.02) [Naive Bayes]\n",
      "Recall: 0.31 (+/- 0.01) [Naive Bayes]\n",
      "Accuracy: 0.85 (+/- 0.00) [Random Forest]\n",
      "Precision: 0.73 (+/- 0.01) [Random Forest]\n",
      "Recall: 0.63 (+/- 0.01) [Random Forest]\n",
      "Accuracy: 0.83 (+/- 0.01) [AdaBoost Decision Tree]\n",
      "Precision: 0.66 (+/- 0.03) [AdaBoost Decision Tree]\n",
      "Recall: 0.62 (+/- 0.02) [AdaBoost Decision Tree]\n",
      "Accuracy: 0.85 (+/- 0.00) [Second Stage Learner]\n",
      "Precision: 0.82 (+/- 0.02) [Second Stage Learner]\n",
      "Recall: 0.50 (+/- 0.02) [Second Stage Learner]\n"
     ]
    }
   ],
   "source": [
    "#Stacking model 3\n",
    "#changed solver = \"saga\" for LogisticRegresssion\n",
    "#changed criterion = \"entropy\" for RandomForest\n",
    "#added class_weight=\"balanced\" back to AdaBoost\n",
    "#changed criterion = \"entropy\" for AdaBoost\n",
    "#NOTE: max_iter increased to 1000 for learner_1 due to convergence error\n",
    "\n",
    "#build/train/validate\n",
    "learner_1 = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000, solver=\"saga\"))\n",
    "learner_2 = GaussianNB()\n",
    "learner_3 = RandomForestClassifier(max_features='auto', n_estimators=100, criterion=\"entropy\")\n",
    "learner_4 = AdaBoostClassifier(tree.DecisionTreeClassifier(criterion=\"entropy\", class_weight=\"balanced\"),\n",
    "                         algorithm=\"SAMME.R\",n_estimators=100)\n",
    "stacked_learner = VotingClassifier(estimators=[('lr', learner_1), ('nb', learner_2),\n",
    "                                              ('rf', learner_3), ('adaboost', learner_4)], voting='hard', n_jobs=-2)\n",
    "for MV, label in zip([learner_1, learner_2, learner_3, learner_4, stacked_learner], \n",
    "                     ['Logistic Regression', 'Naive Bayes', 'Random Forest', 'AdaBoost Decision Tree', 'Second Stage Learner']):\n",
    "    scores_a = cross_val_score(MV, features_train, ttb, cv=5, scoring='accuracy')\n",
    "    scores_p = cross_val_score(MV, features_train, ttb, cv=5, scoring='precision')\n",
    "    scores_r = cross_val_score(MV, features_train, ttb, cv=5, scoring='recall')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores_a.mean(), scores_a.std(), label))\n",
    "    print(\"Precision: %0.2f (+/- %0.2f) [%s]\" % (scores_p.mean(), scores_p.std(), label))\n",
    "    print(\"Recall: %0.2f (+/- %0.2f) [%s]\" % (scores_r.mean(), scores_r.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple parameters were tuned for this final model based on some of the inputs used for the previous individual models. Saga was chosen for the logistic regression learner based on the docs stating that saga is faster for larger datasets. Again, the resulting model had very poor recall at only 50%. The stacked learners had some of the best precision values (85%) but the poorest recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe to store results - this will only store the BEST of the three models based on RECALL (or RECALL + PRECISION)\n",
    "#some code taken from https://stackoverflow.com/questions/18022845/pandas-index-column-title-or-name\n",
    "\n",
    "results = pd.DataFrame(index=['KNN','Decision Tree','Random Forest','SVM-Linear','SVM-RBF','Stochastic Gradient Descent',\n",
    "                             'Adaboost','Bagging Classifier','Gradient Boosting','Extra Trees','ANN','Stacking'],\n",
    "                      columns=['Accuracy(%)','Precision(%)-(>50K)','Recall(%)-(>50K)'])\n",
    "\n",
    "results.index.name = 'Classification Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the results dataframe with model metrics\n",
    "results.iloc[0]=[82.0,65,57]\n",
    "results.iloc[1]=[84.6,72,62]\n",
    "results.iloc[2]=[84.7,72,62]\n",
    "results.iloc[3]=[78.9,54,86]\n",
    "results.iloc[4]=[78.8,54,86]\n",
    "results.iloc[5]=[79.8,56,84]\n",
    "results.iloc[6]=[86.6,77,65]\n",
    "results.iloc[7]=[85.3,74,62]\n",
    "results.iloc[8]=[87.0,77,67]\n",
    "results.iloc[9]=[82.9,67,60]\n",
    "results.iloc[10]=[85.1,72,64]\n",
    "results.iloc[11]=[85.0,78,54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision (&gt;50K)</th>\n",
       "      <th>Recall (&gt;50K)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classification Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>82</td>\n",
       "      <td>65</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>84.6</td>\n",
       "      <td>72</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>84.7</td>\n",
       "      <td>72</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM-Linear</th>\n",
       "      <td>78.9</td>\n",
       "      <td>54</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM-RBF</th>\n",
       "      <td>78.8</td>\n",
       "      <td>54</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stochastic Gradient Descent</th>\n",
       "      <td>79.8</td>\n",
       "      <td>56</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adaboost</th>\n",
       "      <td>86.6</td>\n",
       "      <td>77</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier</th>\n",
       "      <td>85.3</td>\n",
       "      <td>74</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>87</td>\n",
       "      <td>77</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extra Trees</th>\n",
       "      <td>82.9</td>\n",
       "      <td>67</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANN</th>\n",
       "      <td>85.1</td>\n",
       "      <td>72</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stacking</th>\n",
       "      <td>85</td>\n",
       "      <td>78</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Accuracy Precision (>50K) Recall (>50K)\n",
       "Classification Model                                               \n",
       "KNN                               82               65            57\n",
       "Decision Tree                   84.6               72            62\n",
       "Random Forest                   84.7               72            62\n",
       "SVM-Linear                      78.9               54            86\n",
       "SVM-RBF                         78.8               54            86\n",
       "Stochastic Gradient Descent     79.8               56            84\n",
       "Adaboost                        86.6               77            65\n",
       "Bagging Classifier              85.3               74            62\n",
       "Gradient Boosting                 87               77            67\n",
       "Extra Trees                     82.9               67            60\n",
       "ANN                             85.1               72            64\n",
       "Stacking                          85               78            54"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the results table\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table shows the best of three models for each different model type. While the support vector machine and stochastic gradient descent models had the highest recall (>80%) the very low precision (<60%) makes them poor models for predicting the positive class. The gradient boosting model using 1000 estimators was probably the best overall model at 87% accuracy. This model correctly identified 67% of the >50K cases and correctly predicted 77% of them. While this is the \"best\" model I would have ideally liked to have a model with recall of >75% and precision of at least 75% for the >50K class due to the data set being quite unbalanced. None of the models had issues predicting the negative class (<=50K).\n",
    "\n",
    "I think having a more balanced data set would help with future predictions. This data is quite old - taken from a 1994 census - so it would be interesting to predict the same classes on a newer dataset. I think more tuning could have been accomplished as well, had I not been limited by the processing power of my computer. Plugging more parameters into grid search could have helped identify parameters to build better models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
